{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bencottier/cgan-denoiser/blob/master/data_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, wget, zipfile, shutil\n",
    "import warnings, itertools, functools\n",
    "\n",
    "from collections import OrderedDict\n",
    "from argparse import ArgumentParser\n",
    "from skimage import io as io, transform as tfm\n",
    "\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "mpl.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "mpl.rcParams[\"axes.grid\"     ] = False\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Conv2d as Conv, ConvTranspose2d as Deconv,  ReLU as Relu\n",
    "from torch.nn import InstanceNorm2d as InstanceNorm, BatchNorm2d as BatchNorm\n",
    "from torch.utils.tensorboard import SummaryWriter,  FileWriter,  RecordWriter\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, Callback, ModelCheckpoint\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from Imports import *\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, image_size: (int, tuple) = 256):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            image_size: Final size of the image\n",
    "        \"\"\"\n",
    "\n",
    "        if   isinstance(image_size, int):   self.image_size = (image_size, image_size)\n",
    "        elif isinstance(image_size, tuple): self.image_size = image_size\n",
    "        else: raise ValueError(\"Unknown DataType of the parameter image_size found!!\")\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "\n",
    "        A = tfm.resize(A, output_shape = self.image_size)\n",
    "        B = tfm.resize(B, output_shape = self.image_size)\n",
    "\n",
    "        A = np.clip(A, a_min = 0., a_max = 1.)\n",
    "        B = np.clip(B, a_min = 0., a_max = 1.)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "\n",
    "    def __init__(self, image_size: (int, tuple) = 256):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            image_size: Final size of the image (should be smaller than current size o/w returns the original image)\n",
    "        \"\"\"\n",
    "\n",
    "        if   isinstance(image_size, int):   self.image_size = (image_size, image_size)\n",
    "        elif isinstance(image_size, tuple): self.image_size = image_size\n",
    "        else: raise ValueError(\"Unknown DataType of the parameter image_size found!!\")\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "        curr_height, curr_width = A.shape[0], A.shape[1]\n",
    "\n",
    "        ht_diff = max(0, curr_height - self.image_size[0])\n",
    "        wd_diff = max(0, curr_width  - self.image_size[1])\n",
    "        top = np.random.randint(low = 0, high = ht_diff)\n",
    "        lft = np.random.randint(low = 0, high = wd_diff)\n",
    "\n",
    "        A = A[top: top + self.image_size[0], lft: lft + self.image_size[1]]\n",
    "        B = B[top: top + self.image_size[0], lft: lft + self.image_size[1]]\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class Random_Flip(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "        if np.random.uniform(low = 0., high = 1.0) > .5:\n",
    "            A = np.fliplr(A)\n",
    "            B = np.fliplr(B)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class To_Tensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A = np.transpose(sample['A'].astype(np.float, copy = True), (2, 0, 1))\n",
    "        B = np.transpose(sample['B'].astype(np.float, copy = True), (2, 0, 1))\n",
    "\n",
    "        A = torch.tensor(A, dtype = torch.float)\n",
    "        B = torch.tensor(B, dtype = torch.float)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "\n",
    "    def __init__(self, mean = [0.5] * 3, stdv = [0.5] * 3):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            mean: Normalizing mean\n",
    "            stdv: Normalizing stdv\n",
    "        \"\"\"\n",
    "\n",
    "        mean = torch.tensor(mean, dtype = torch.float)\n",
    "        stdv = torch.tensor(stdv, dtype = torch.float)\n",
    "        self.transforms = T.Normalize(mean = mean, std = stdv)\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "        A = self.transforms(A)\n",
    "        B = self.transforms(B)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = None, transforms = None, max_sz: int = 1000):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            transforms: a list of Transformations (Data augmentation)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(); self.transforms = T.Compose(transforms)\n",
    "\n",
    "        file_names_A = sorted(os.listdir(path + 'A/'), key = lambda x: int(x[: -4]))\n",
    "        self.file_names_A = [path + 'A/' + file_name for file_name in file_names_A]\n",
    "\n",
    "        file_names_B = sorted(os.listdir(path + 'B/'), key = lambda x: int(x[: -4]))\n",
    "        self.file_names_B = [path + 'B/' + file_name for file_name in file_names_B]\n",
    "\n",
    "        self.file_names_A = self.file_names_A[:max_sz]\n",
    "        self.file_names_B = self.file_names_B[:max_sz]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.file_names_A) == len(self.file_names_B)\n",
    "        return len(self.file_names_A)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        A = io.imread(self.file_names_A[idx])\n",
    "        B = io.imread(self.file_names_B[idx])\n",
    "        sample = self.transforms({'A': A, 'B': B})\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    \"\"\"\n",
    "    Download the dataset using the below link; you just need to specify the url while creating an object of this class\n",
    "    https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\n",
    "    Authors don't follow a consistent format for all the datasets, so, it might not work for few\n",
    "\n",
    "    Implements the Lightining DataModule!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url: str, root_dir: str = \"./Dataset/CycleGAN/\", img_sz: int = 256, trn_batch_sz: int = 4,\n",
    "                 tst_batch_sz: int = 64):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            url:          Download URL of the dataset\n",
    "            root_dir:     Root dir where dataset needs to be downloaded\n",
    "            img_sz:       Size of the Image\n",
    "            trn_batch_sz: Training Batch Size\n",
    "            tst_batch_sz: Test Batch Size\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.url = url\n",
    "        self.dataset = url.split(\"/\")[-1]\n",
    "\n",
    "        self.processed_dir  = root_dir + \"Processed/\"\n",
    "        self.compressed_dir = root_dir + \"Compressed/\"\n",
    "        os.makedirs(self.processed_dir , exist_ok = True)\n",
    "        os.makedirs(self.compressed_dir, exist_ok = True)\n",
    "\n",
    "        self.trn_batch_sz = trn_batch_sz\n",
    "        self.tst_batch_sz = tst_batch_sz\n",
    "\n",
    "        jitter_sz = int(img_sz * 1.120)\n",
    "        self.tst_tfms = [Resize(img_sz), To_Tensor(), Normalize()]\n",
    "        self.trn_tfms = [Resize(jitter_sz), RandomCrop(img_sz), Random_Flip(), To_Tensor(), Normalize()]\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        if self.dataset in os.listdir(self.compressed_dir):\n",
    "            print(f\"Dataset {self.dataset[:-4]} already exists!\")\n",
    "        else:\n",
    "            print(f\"Downloading dataset {self.dataset[:-4]}!!\")\n",
    "            wget.download(self.url, self.compressed_dir)\n",
    "            print(f\"\\nDataset {self.dataset[:-4]} downloaded. Extraction in progress!\")\n",
    "\n",
    "            with zipfile.ZipFile(self.compressed_dir + self.dataset, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.processed_dir)\n",
    "            print(f\"Extraction done!\")\n",
    "\n",
    "            # you might need to modify the below code; it's not generic, but works for most of the datasets \n",
    "            # listed in that url.\n",
    "            \n",
    "            dwnld_dir = self.processed_dir + self.dataset[:-4] + \"/\"\n",
    "            for folder in [\"testA/\", \"testB/\", \"trainA/\", \"trainB/\"]:\n",
    "\n",
    "                dest_dir = dwnld_dir\n",
    "                src_dir  = dwnld_dir + folder\n",
    "\n",
    "                dest_dir = dest_dir + \"Train/\" if folder[:-2] != \"test\" else dest_dir + \"Test/\"\n",
    "                dest_dir = dest_dir + \"B/\"     if folder[-2]  != \"A\"    else dest_dir + \"A/\"\n",
    "                os.makedirs(dest_dir, exist_ok = True)\n",
    "\n",
    "                orig_files = [src_dir  + file for file in sorted(os.listdir(src_dir))]\n",
    "                modf_files = [dest_dir + \"{:06d}.jpg\".format(i) for i, file in enumerate(orig_files)]\n",
    "\n",
    "                for orig_file, modf_file in zip(orig_files, modf_files):\n",
    "                    shutil.move(orig_file, modf_file)\n",
    "                os.rmdir(src_dir)\n",
    "\n",
    "            print(f\"Files moved to appropiate folder!\")\n",
    "\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        \"\"\"\n",
    "        stage: fit/test\n",
    "        \"\"\"\n",
    "\n",
    "        dwnld_dir = self.processed_dir + self.dataset[:-4]\n",
    "        trn_dir = dwnld_dir + \"/Train/\"\n",
    "        tst_dir = dwnld_dir + \"/Test/\"\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "\n",
    "            dataset = CustomDataset(path = trn_dir, transforms = self.trn_tfms)\n",
    "            train_sz = int(len(dataset) * 0.9)\n",
    "            valid_sz = len(dataset) - train_sz\n",
    "\n",
    "            self.train, self.valid = random_split(dataset, [train_sz, valid_sz])\n",
    "            print(f\"Size of the training dataset: {train_sz}, validation dataset: {valid_sz}\")\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test = CustomDataset(path = tst_dir, transforms = self.tst_tfms)\n",
    "            print(f\"Size of the test dataset: {len(self.test)}\")\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size = self.trn_batch_sz, shuffle = True , num_workers = 16, \n",
    "                          pin_memory = True)\n",
    "\n",
    "    def val_dataloader  (self):\n",
    "        return DataLoader(self.valid, batch_size = self.tst_batch_sz, shuffle = False, num_workers = 16, \n",
    "                          pin_memory = True)\n",
    "\n",
    "    def test_dataloader (self):\n",
    "        return DataLoader(self.test , batch_size = self.tst_batch_sz, shuffle = False, num_workers = 16, \n",
    "                          pin_memory = True)\n",
    "\n",
    "\n",
    "def show_image(image):\n",
    "    plt.imshow(np.transpose((image + 1) / 2, (1, 2, 0)))\n",
    "\n",
    "def get_random_sample(dataset):\n",
    "    return dataset[np.random.randint(0, len(dataset))]\n",
    "\n",
    "\n",
    "###############################################################################################################################################\n",
    "\n",
    "\n",
    "img_sz = 256\n",
    "url = \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/facades.zip\"\n",
    "\n",
    "# You can decrease the num_workers argument in {train/val/test}_dataloader\n",
    "# datamodule = DataModule(url, root_dir = \"./Dataset/Pix2Pix/\", trn_batch_sz = 1, tst_batch_sz = 64)\n",
    "# datamodule.prepare_data()\n",
    "# datamodule.setup(\"fit\")\n",
    "\n",
    "\n",
    "# print(f\"Few random samples from the Training dataset!\")\n",
    "\n",
    "# sample = get_random_sample(datamodule.train)\n",
    "# plt.subplot(1, 2, 1); show_image(sample['A'])\n",
    "# plt.subplot(1, 2, 2); show_image(sample['B'])\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Few random samples from the Validation dataset!\")\n",
    "\n",
    "# sample = get_random_sample(datamodule.valid)\n",
    "# plt.subplot(1, 2, 1); show_image(sample['A'])\n",
    "# plt.subplot(1, 2, 2); show_image(sample['B'])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "###############################################################################################################################################\n",
    "\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels: int, inner_channels: int, innermost: bool = False, outermost: bool = False,\n",
    "                 apply_dp: bool = False, submodule = None, add_skip_conn: bool = True, norm_type: str = 'instance'):\n",
    "\n",
    "\n",
    "        \"\"\"Defines a Unet submodule with/without skip connection!\n",
    "        X -----------------identity(optional)--------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_channels: Number of output channels in the DeConvolutional layer\n",
    "            inner_channels: Number of output channels in the Convolutional layer\n",
    "            innermost:      If this module is the innermost module\n",
    "            outermost:      If this module is the outermost module\n",
    "            apply_dp:       If apply_dp is set to True, then activations are 0'ed out with prob 0.5\n",
    "            submodule:      Previously defined UNet submodule\n",
    "            add_skip_conn:  If set to true, skip connections are added b/w Encoder and Decoder\n",
    "            norm_type:      Type of Normalization layer - InstanceNorm2D or BatchNorm2D\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.outermost = outermost\n",
    "        self.add_skip_conn = add_skip_conn\n",
    "\n",
    "        bias = norm_type == 'instance'\n",
    "        f = 2 if add_skip_conn else 1\n",
    "        norm_layer = InstanceNorm if norm_type == 'instance' else BatchNorm\n",
    "\n",
    "        if  innermost:\n",
    "            dn_conv = Conv  (in_channels = input_channels, out_channels = inner_channels, kernel_size = 4, stride = 2,\n",
    "                             padding = 1, bias = True, padding_mode = 'zeros')\n",
    "            up_conv = Deconv(in_channels = inner_channels, out_channels = input_channels, kernel_size = 4, stride = 2,\n",
    "                             padding = 1, bias = bias, padding_mode = 'zeros')\n",
    "\n",
    "            dn_layers = [nn.LeakyReLU(0.2, True), dn_conv]\n",
    "            up_layers = [nn.ReLU(True), up_conv, norm_layer(input_channels)]\n",
    "            layers = dn_layers + up_layers\n",
    "\n",
    "        elif outermost:\n",
    "            dn_conv = Conv  (in_channels = 1 * input_channels, out_channels = inner_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = True, padding_mode = 'zeros')\n",
    "            up_conv = Deconv(in_channels = f * inner_channels, out_channels = input_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = True, padding_mode = 'zeros')\n",
    "\n",
    "            dn_layers = [dn_conv]\n",
    "            up_layers = [nn.ReLU(True), up_conv, nn.Tanh()]\n",
    "            layers = dn_layers + [submodule] + up_layers\n",
    "\n",
    "        else:\n",
    "            dn_conv = Conv  (in_channels = 1 * input_channels, out_channels = inner_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = bias, padding_mode = 'zeros')\n",
    "            up_conv = Deconv(in_channels = f * inner_channels, out_channels = input_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = bias, padding_mode = 'zeros')\n",
    "\n",
    "            dn_layers = [nn.LeakyReLU(0.2, True), dn_conv, norm_layer(inner_channels)]\n",
    "            up_layers = [nn.ReLU(True), up_conv, norm_layer(input_channels)]\n",
    "\n",
    "            if apply_dp:\n",
    "                layers = dn_layers + [submodule] + up_layers + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                layers = dn_layers + [submodule] + up_layers\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.outermost: return self.net(x)\n",
    "        else: return torch.cat([x, self.net(x)], dim = 1) if self.add_skip_conn else self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, out_channels: int = 64, nb_layers: int = 8, apply_dp: bool = True,\n",
    "                 add_skip_conn: bool = True, norm_type: str = 'instance'):\n",
    "\n",
    "        \"\"\"\n",
    "                            Generator Architecture!\n",
    "        Encoder:        C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        U-Net Decoder:  CD1024-CD1024-CD1024-CD1024-CD512-CD256-CD128, where Ck denote a Convolution-InsNorm-ReLU\n",
    "        layer with k filters, and CDk denotes a Convolution-InsNorm-Dropout-ReLU layer with a dropout rate of 50%\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            in_channels:    Number of input channels\n",
    "            out_channels:   Number of output channels\n",
    "            nb_layers:      Number of layers in the Generator\n",
    "            apply_dp:       If apply_dp is set to True, then activations are 0'ed out with prob \"drop_param\"\n",
    "            add_skip_conn:  If set to true, skip connections are added b/w Encoder and Decoder\n",
    "            norm_type:      Type of Normalization layer - InstanceNorm2D or BatchNorm2D\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        f = 4\n",
    "        self.layers = []\n",
    "\n",
    "        unet = UNetBlock(out_channels * 8, out_channels * 8, innermost = True, outermost = False, apply_dp = False,\n",
    "                         submodule = None, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "\n",
    "        for idx in range(nb_layers - 5):\n",
    "            unet = UNetBlock(out_channels * 8, out_channels * 8, innermost = False, outermost = False, apply_dp =\n",
    "                             apply_dp, submodule = unet, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "\n",
    "        for idx in range(0, 3):\n",
    "            unet = UNetBlock(out_channels * f, out_channels*2*f, innermost = False, outermost = False, apply_dp =\n",
    "                             False,    submodule = unet, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "            f = f // 2\n",
    "\n",
    "        unet = UNetBlock(in_channels * 1, out_channels * 1, innermost = False, outermost = True,  apply_dp = False,\n",
    "                         submodule = unet, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "\n",
    "        self.net = unet\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, nb_layers = 3, norm_type: str = 'instance'):\n",
    "\n",
    "        \"\"\"\n",
    "                                    Discriminator Architecture!\n",
    "        C64 - C128 - C256 - C512, where Ck denote a Convolution-InstanceNorm-LeakyReLU layer with k filters\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            in_channels:    Number of input channels\n",
    "            out_channels:   Number of output channels\n",
    "            nb_layers:      Number of layers in the 70*70 Patch Discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        in_f  = 1\n",
    "        out_f = 2\n",
    "        bias = norm_type == 'instance'\n",
    "        norm_layer = InstanceNorm if norm_type == \"instance\" else BatchNorm\n",
    "\n",
    "        conv = Conv(in_channels, out_channels, 4, stride = 2, padding = 1, bias = True)\n",
    "        layers = [conv, nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        for idx in range(1, nb_layers):\n",
    "            conv = Conv(out_channels * in_f, out_channels * out_f, 4, stride = 2, padding = 1, bias = bias)\n",
    "            layers += [conv, norm_layer(out_channels * out_f), nn.LeakyReLU(0.2, True)]\n",
    "            in_f = out_f; out_f *= 2\n",
    "\n",
    "        out_f = min(2 ** nb_layers, 8)\n",
    "        conv = Conv(out_channels * in_f, out_channels * out_f, 4, stride = 1, padding = 1, bias = bias)\n",
    "        layers += [conv, norm_layer(out_channels * out_f), nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        conv = Conv(out_channels * out_f, 1, 4, stride = 1, padding = 1, bias = True)\n",
    "        layers += [conv]\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Initializer:\n",
    "\n",
    "    def __init__(self, init_type: str = 'normal', init_gain: float = 0.02):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            init_type: Initializer type - 'kaiming' or 'xavier' or 'normal'\n",
    "            init_gain: Standard deviation of the normal distribution\n",
    "        \"\"\"\n",
    "\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "\n",
    "    def init_module(self, m):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            m: Module\n",
    "        \"\"\"\n",
    "\n",
    "        cls_name = m.__class__.__name__;\n",
    "        if hasattr(m, 'weight') and (cls_name.find('Conv') != -1 or cls_name.find('Linear') != -1):\n",
    "\n",
    "            if   self.init_type == 'kaiming': nn.init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
    "            elif self.init_type == 'xavier' : nn.init.xavier_normal_ (m.weight.data,  gain = self.init_gain)\n",
    "            elif self.init_type == 'normal' : nn.init.normal_(m.weight.data, mean = 0, std = self.init_gain)\n",
    "            else: raise ValueError('Initialization not found!!')\n",
    "\n",
    "            if m.bias is not None: nn.init.constant_(m.bias.data, val = 0);\n",
    "\n",
    "        if hasattr(m, 'weight') and cls_name.find('BatchNorm2d') != -1:\n",
    "            nn.init.normal_(m.weight.data, mean = 1.0, std = self.init_gain)\n",
    "            nn.init.constant_(m.bias.data, val = 0)\n",
    "\n",
    "\n",
    "    def __call__(self, net):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            net: Network\n",
    "        \"\"\"\n",
    "        \n",
    "        net.apply(self.init_module)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "\n",
    "class Loss:\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements different losses required to train the generators and discriminators of CycleGAN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_type: str = 'MSE', lambda_: int = 100):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            loss_type: Loss Function to train CycleGAN\n",
    "            lambda_:   Weightage of Cycle-consistency loss\n",
    "        \"\"\"\n",
    "\n",
    "        self.loss = nn.MSELoss() if loss_type == 'MSE' else nn.BCEWithLogitsLoss()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "\n",
    "    def get_dis_loss(self, dis_pred_real_data, dis_pred_fake_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dis_pred_real_data: Discriminator's prediction on real data\n",
    "            dis_pred_fake_data: Discriminator's prediction on fake data\n",
    "        \"\"\"\n",
    "\n",
    "        dis_tar_real_data = torch.ones_like (dis_pred_real_data, requires_grad = False)\n",
    "        dis_tar_fake_data = torch.zeros_like(dis_pred_fake_data, requires_grad = False)\n",
    "\n",
    "        loss_real_data = self.loss(dis_pred_real_data, dis_tar_real_data)\n",
    "        loss_fake_data = self.loss(dis_pred_fake_data, dis_tar_fake_data)\n",
    "\n",
    "        dis_tot_loss = (loss_real_data + loss_fake_data) * 0.5\n",
    "\n",
    "        return dis_tot_loss\n",
    "\n",
    "\n",
    "    def get_gen_gan_loss(self, dis_pred_fake_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dis_pred_fake_data: Discriminator's prediction on fake data\n",
    "        \"\"\"\n",
    "\n",
    "        gen_tar_fake_data = torch.ones_like(dis_pred_fake_data, requires_grad = False)\n",
    "        gen_tot_loss = self.loss(dis_pred_fake_data, gen_tar_fake_data)\n",
    "\n",
    "        return gen_tot_loss\n",
    "\n",
    "\n",
    "    def get_gen_rec_loss(self, real_data, recs_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            real_data: Real images sampled from the dataloaders\n",
    "            recs_data: Fake label generated by the generator\n",
    "        \"\"\"\n",
    "\n",
    "        gen_rec_loss = torch.nn.L1Loss()(real_data, recs_data)\n",
    "        gen_tot_loss = gen_rec_loss * self.lambda_\n",
    "\n",
    "        return gen_tot_loss\n",
    "\n",
    "\n",
    "    def get_gen_loss(self, dis_pred_fake_data, real_data, fake_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Implements the total Generator loss\n",
    "        Sum of Reconstruction loss, and GAN loss\n",
    "        \"\"\"\n",
    "\n",
    "        gen_gan_loss = self.get_gen_gan_loss(dis_pred_fake_data  )\n",
    "        gen_rec_loss = self.get_gen_rec_loss(real_data, fake_data)\n",
    "        gen_tot_loss = gen_gan_loss + gen_rec_loss\n",
    "\n",
    "        return gen_tot_loss\n",
    "\n",
    "\n",
    "\n",
    "class Pix2Pix(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, d_lr: float = 2e-4, g_lr: float = 2e-4, beta_1: float = 0.5, beta_2: float = 0.999, epoch_decay: int = 100):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_lr = d_lr\n",
    "        self.g_lr = g_lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epoch_decay = epoch_decay\n",
    "\n",
    "        self.loss = Loss(loss_type = 'MSE', lambda_ = 100)\n",
    "        init = Initializer(init_type = 'normal', init_gain = 0.02)\n",
    "        \n",
    "        self.gen  = init(Generator(in_channels = 3, out_channels = 64, norm_type = 'instance'))\n",
    "        self.dis  = init(Discriminator(in_channels = 3, out_channels = 64, norm_type = 'instance'))\n",
    "\n",
    "        self.d_params = self.dis.parameters()\n",
    "        self.g_params = self.gen.parameters()\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        self.example_input_array = torch.rand(1, 3, img_sz, img_sz, device = self.device)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def set_requires_grad(nets, requires_grad = False):\n",
    "\n",
    "        \"\"\"\n",
    "        Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "        Parameters:\n",
    "            nets (network list)   -- a list of networks\n",
    "            requires_grad (bool)  -- whether the networks require gradients or not\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(nets, list): nets = [nets]\n",
    "        for net in nets:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "    def forward(self, real_A):\n",
    "        \n",
    "        # this is different from the training step. You should treat this as the final inference code (final outputs that you are looking for!)\n",
    "        fake_B = self.gen(real_A)\n",
    "\n",
    "        return fake_B\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        real_A, real_B = batch['A'], batch['B']\n",
    "        fake_B = self.gen(real_A)\n",
    "\n",
    "        if batch_idx % 2 == 0:\n",
    "\n",
    "            # No need to calculate the gradients for Discriminators' parameters\n",
    "            self.set_requires_grad([self.dis], requires_grad = False)\n",
    "            dis_pred_fake_data = self.dis(torch.cat([real_A, fake_B], 0))\n",
    "            \n",
    "            # Gen loss\n",
    "            g_loss = self.loss.get_gen_loss(dis_pred_fake_data, real_B, fake_B)\n",
    "            self.log(\"g_train_loss\", g_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "            \n",
    "            return g_loss\n",
    "\n",
    "\n",
    "        else:\n",
    "            \n",
    "            self.set_requires_grad([self.dis], requires_grad = True)\n",
    "            dis_pred_real_data = self.dis(torch.cat([real_A, real_B], 0))\n",
    "            dis_pred_fake_data = self.dis(torch.cat([real_A, fake_B.detach()], 0))\n",
    "\n",
    "            # Dis loss\n",
    "            d_loss = self.loss.get_dis_loss(dis_pred_real_data, dis_pred_fake_data)\n",
    "            self.log(\"d_train_loss\", d_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "\n",
    "            return d_loss\n",
    "\n",
    "\n",
    "    def shared_step(self, batch, stage: str = 'val'):\n",
    "\n",
    "        grid = []\n",
    "        real_A, real_B = batch['A'], batch['B']\n",
    "\n",
    "        fake_B = self.gen(real_A)\n",
    "        dis_pred_fake_data = self.dis(torch.cat([real_A, fake_B], 0))\n",
    "        dis_pred_real_data = self.dis(torch.cat([real_A, real_B], 0))\n",
    "\n",
    "        # Gen loss, # Dis loss\n",
    "        g_loss = self.loss.get_gen_loss(dis_pred_fake_data, real_B, fake_B)\n",
    "        d_loss = self.loss.get_dis_loss(dis_pred_real_data, dis_pred_fake_data)\n",
    "\n",
    "        dict_ = {f'g_{stage}_loss': g_loss, f'd_{stage}_loss': d_loss}\n",
    "        self.log_dict(dict_, on_step = False, on_epoch = True, prog_bar = True, logger = True)\n",
    "\n",
    "        for i in range(12):\n",
    "            rand_int = np.random.randint(0, len(real_A))\n",
    "            tensor = torch.stack([real_A[rand_int], fake_B[rand_int], real_B[rand_int]])\n",
    "            grid.append((tensor + 1) / 2)\n",
    "            \n",
    "        # log the results on tensorboard\n",
    "        grid = torchvision.utils.make_grid(torch.cat(grid, 0), nrow = 6)\n",
    "        self.logger.experiment.add_image('Grid', grid, self.current_epoch, dataformats = \"CHW\")\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, 'val')\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, 'test')\n",
    "\n",
    "\n",
    "    def lr_lambda(self, epoch):\n",
    "\n",
    "        fraction = (epoch - self.epoch_decay) / self.epoch_decay\n",
    "        return 1 if epoch < self.epoch_decay else 1 - fraction\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # define the optimizers here\n",
    "        g_opt = torch.optim.Adam(self.g_params, lr = self.g_lr, betas = (self.beta_1, self.beta_2))\n",
    "        d_opt = torch.optim.Adam(self.d_params, lr = self.d_lr, betas = (self.beta_1, self.beta_2))\n",
    "        \n",
    "        # define the lr_schedulers here\n",
    "        g_sch = optim.lr_scheduler.LambdaLR(g_opt, lr_lambda = self.lr_lambda)\n",
    "        d_sch = optim.lr_scheduler.LambdaLR(d_opt, lr_lambda = self.lr_lambda)\n",
    "        \n",
    "        # first return value is a list of optimizers and second is a list of lr_schedulers (you can return empty list also)\n",
    "        return [g_opt, d_opt], [g_sch, d_sch]\n",
    "\n",
    "\n",
    "    \n",
    "###############################################################################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Datasets' already exists.\n",
      "Collecting image paths...\n",
      "NOT HELDOUT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 5948.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataLoader from .npy file...\n",
      "Loading DataLoader from .npy file...\n",
      "DataLoader loaded from .npy files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from DataLoaderManager import DataLoaderManager\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 128\n",
    "# Example usage:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "\n",
    "target_size = None\n",
    "lr=0.0002\n",
    "lambda_recon=200\n",
    "# batch_size = 128\n",
    "\n",
    "\n",
    "display_step = 250\n",
    "\n",
    "root_dir='SIDD_Small_sRGB/SIDD_Small_sRGB/Data/'\n",
    "base_filename = 'SIDD_Small_sRGB'\n",
    "\n",
    "data_loader_manager = DataLoaderManager(root_dir=root_dir, train_file=base_filename,test_file=base_filename,make_held_out_set=False,transform=transform)\n",
    "dataloader, val_dataloader = data_loader_manager.process_dataloaders(batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type          | Params | In sizes         | Out sizes       \n",
      "-----------------------------------------------------------------------------\n",
      "0 | gen  | Generator     | 54.4 M | [1, 3, 256, 256] | [1, 3, 256, 256]\n",
      "1 | dis  | Discriminator | 2.8 M  | ?                | ?               \n",
      "-----------------------------------------------------------------------------\n",
      "57.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "57.2 M    Total params\n",
      "228.697   Total estimated model params size (MB)\n",
      "c:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\torch\\jit\\_trace.py:1093: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the repeated trace. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 8362 / 196608 (4.3%)\n",
      "Greatest absolute difference: 9.420514106750488e-05 at index (0, 0, 197, 199) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 0.6414586619652964 at index (0, 0, 114, 55) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "c:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3266cd6e7e402f8a774c55be6fb293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39m# checkpoint_callback = ModelCheckpoint(monitor = \"g_val_loss\", save_top_k = 3, period = 2, save_last = True)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[39m# callbacks = [lr_logger, checkpoint_callback]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \n\u001b[0;32m     19\u001b[0m     \u001b[39m# you can change the gpus argument to how many you have (I had only 1 :( )\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[39m# Setting deterministic flag to True for full reproducibility\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs \u001b[39m=\u001b[39m epochs, precision \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m, num_sanity_val_steps \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, logger \u001b[39m=\u001b[39m tb_logger)\n\u001b[1;32m---> 23\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model, dataloader)\n\u001b[0;32m     26\u001b[0m \u001b[39mif\u001b[39;00m TEST:\n\u001b[0;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    This is one of the many ways to run inference, but I would recommend you to look into the docs for other \u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    options as well, so that you can use one which suits you best.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    546\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1033\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1034\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    139\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    141\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:252\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    250\u001b[0m             batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautomatic_optimization\u001b[39m.\u001b[39mrun(trainer\u001b[39m.\u001b[39moptimizers[\u001b[39m0\u001b[39m], batch_idx, kwargs)\n\u001b[0;32m    251\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m             batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmanual_optimization\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    256\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:94\u001b[0m, in \u001b[0;36m_ManualOptimization.run\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_start()\n\u001b[0;32m     93\u001b[0m \u001b[39mwith\u001b[39;00m suppress(\u001b[39mStopIteration\u001b[39;00m):  \u001b[39m# no loop to break at this level\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(kwargs)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_end()\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:114\u001b[0m, in \u001b[0;36m_ManualOptimization.advance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[0;32m    113\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    115\u001b[0m \u001b[39mdel\u001b[39;00m kwargs  \u001b[39m# release the batch from memory\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[0;32m    390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mtraining_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[17], line 675\u001b[0m, in \u001b[0;36mPix2Pix.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m--> 675\u001b[0m     real_A, real_B \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m], batch[\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    676\u001b[0m     fake_B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen(real_A)\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    679\u001b[0m \n\u001b[0;32m    680\u001b[0m         \u001b[39m# No need to calculate the gradients for Discriminators' parameters\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "TEST    = True\n",
    "TRAIN   = True\n",
    "RESTORE = False\n",
    "resume_from_checkpoint = None if TRAIN else \"path/to/checkpoints/\" # \"./logs/Pix2Pix/version_0/checkpoints/epoch=1.ckpt\"\n",
    "\n",
    "\n",
    "if TRAIN or RESTORE:\n",
    "    \n",
    "    epochs = 200\n",
    "    epoch_decay = epochs // 2\n",
    "    \n",
    "    model = Pix2Pix(epoch_decay = epoch_decay)\n",
    "    tb_logger = pl_loggers.TensorBoardLogger('logs/', name = \"Pix2Pix\", log_graph = True)\n",
    "    \n",
    "    lr_logger = LearningRateMonitor(logging_interval = 'epoch')\n",
    "    # checkpoint_callback = ModelCheckpoint(monitor = \"g_val_loss\", save_top_k = 3, period = 2, save_last = True)\n",
    "    # callbacks = [lr_logger, checkpoint_callback]\n",
    "    \n",
    "    # you can change the gpus argument to how many you have (I had only 1 :( )\n",
    "    # Setting deterministic flag to True for full reproducibility\n",
    "    trainer = pl.Trainer(max_epochs = epochs, precision = 16, num_sanity_val_steps = 1, logger = tb_logger)\n",
    "    \n",
    "    trainer.fit(model, dataloader)\n",
    "    \n",
    "    \n",
    "if TEST:\n",
    "    \n",
    "    \"\"\"\n",
    "    This is one of the many ways to run inference, but I would recommend you to look into the docs for other \n",
    "    options as well, so that you can use one which suits you best.\n",
    "    \"\"\"\n",
    "    \n",
    "    trainer = pl.Trainer(gpus = -1, precision = 16, profiler = True)\n",
    "    # load the checkpoint that you want to load\n",
    "    checkpoint_path = \"path/to/checkpoints/\" # \"./logs/Pix2Pix/version_0/checkpoints/epoch=1.ckpt\"\n",
    "    \n",
    "    model = Pix2Pix.load_from_checkpoint(checkpoint_path = checkpoint_path)\n",
    "    model.freeze()\n",
    "    \n",
    "    # put the datamodule in test mode\n",
    "    datamodule.setup(\"test\")\n",
    "    test_data = datamodule.test_dataloader()\n",
    "\n",
    "    trainer.test(model, test_dataloaders = test_data)\n",
    "    # look tensorboard for the final results\n",
    "    # You can also run an inference on a single image using the forward function defined above!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
