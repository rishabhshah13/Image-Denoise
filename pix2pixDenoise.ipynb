{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# Small SIDD dataset\n",
    "# !kaggle datasets download -d rajat95gupta/smartphone-image-denoising-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip 'smartphone-image-denoising-dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataload\n",
    "# augment\n",
    "# visualize\n",
    "# train\n",
    "# save model\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "n_epochs = 200\n",
    "dataset_name = \"SIDD_Small_sRGB_Only\"\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "channels = 3\n",
    "sample_interval = 500\n",
    "checkpoint_interval = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "REBUILD_DATA=True\n",
    "\n",
    "class Denoising():\n",
    "    IMG_SIZE=256 \n",
    "    label_dir='SIDD_Small_sRGB_Only/Data/' #dataset directory\n",
    "    training_data=[]\n",
    "    validation_data=[]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_images, self.val_images = train_test_split(os.listdir(self.label_dir)[:20], test_size=0.2, random_state=42)\n",
    "\n",
    "    def make_training_and_validation_data(self):\n",
    "        print(self.train_images)\n",
    "        print(self.val_images)\n",
    "\n",
    "        \n",
    "        for image_folder in tqdm(os.listdir(self.label_dir)[:20]):\n",
    "            # print(image_folder)\n",
    "            for img_path in glob(self.label_dir + image_folder + '/*'):\n",
    "                try:\n",
    "                    img=cv2.imread(img_path)\n",
    "                    img=cv2.resize(img,(self.IMG_SIZE,self.IMG_SIZE))\n",
    "                    \n",
    "                    # Check if the image belongs to train or validation set\n",
    "                    if image_folder in self.train_images:\n",
    "                        self.training_data.append(np.array(img))\n",
    "                    elif image_folder in self.val_images:\n",
    "                        self.validation_data.append(np.array(img))\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "        np.save('SIDD_Small_sRGB_Only_training_data.npy',self.training_data)\n",
    "        np.save('SIDD_Small_sRGB_Only_validation_data.npy',self.validation_data)\n",
    "        \n",
    "# if (REBUILD_DATA):\n",
    "#     denoising=Denoising()\n",
    "#     denoising.make_training_and_validation_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pickle file\n",
    "training_data=np.load('SIDD_Small_sRGB_Only_training_data.npy',allow_pickle=True)\n",
    "val_data=np.load('SIDD_Small_sRGB_Only_validation_data.npy',allow_pickle=True)\n",
    "len(training_data), len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(data, vis_num):\n",
    "    num_cols = 2\n",
    "    num_rows = vis_num\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            index = i * num_cols + j\n",
    "            axs[i, j].imshow(data[index])\n",
    "            axs[i, j].set_title('Original' if index % 2 == 0 else 'Noisy')\n",
    "            axs[i, j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Assuming you have already loaded training_data and val_data\n",
    "vis_num = 0\n",
    "display_images(training_data, vis_num)\n",
    "display_images(val_data, vis_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pix2pix.py --train_dataset_loc 'SIDD_Small_sRGB_Only_training_data.npy' --test_dataset_loc 'SIDD_Small_sRGB_Only_validation_data.npy' --n_epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_pixelwise.cuda()\n",
    "\n",
    "if epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (dataset_name, epoch)))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/%s/discriminator_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1,b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "\n",
    "\n",
    "from DataLoader import CustomDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image\n",
    "    transforms.ToTensor(),           # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Set root directory\n",
    "# root_dir = 'data'\n",
    "root_dir='SIDD_Small_sRGB_Only/Data/' #dataset directory\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(root_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataloaders for train and test sets\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Tensor type\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A = Variable(batch[0].type(Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_B = generator(real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(epoch, 2 ):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[0].type(Tensor))\n",
    "        real_B = Variable(batch[1].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python pix2pix.py --train_dataset_loc 'SIDD_Small_sRGB_Only_training_data.npy' --test_dataset_loc 'SIDD_Small_sRGB_Only_validation_data.npy' --n_epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define loss functions\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to the device\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "lr = 0.0002\n",
    "size = 256\n",
    "\n",
    "# Define dataset and dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.CenterCrop(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# dataset = ImageFolder(root='dataset_path', transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, data in enumerate(dataloader):\n",
    "        print(i)\n",
    "        real_images, _ = data\n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # Train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        fake_images = generator(real_images)\n",
    "        \n",
    "        # Train discriminator with real images\n",
    "        real_validity = discriminator(torch.cat((real_images, real_images), 1))\n",
    "        real_labels = torch.ones(real_validity.size()).to(device)\n",
    "        real_loss = criterion_GAN(real_validity, real_labels)\n",
    "        \n",
    "        # Train discriminator with fake images\n",
    "        fake_validity = discriminator(torch.cat((real_images, fake_images.detach()), 1))\n",
    "        fake_labels = torch.zeros(fake_validity.size()).to(device)\n",
    "        fake_loss = criterion_GAN(fake_validity, fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        discriminator_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Adversarial loss\n",
    "        validity = discriminator(torch.cat((real_images, fake_images), 1))\n",
    "        target_labels = torch.ones(validity.size()).to(device)\n",
    "        adversarial_loss = criterion_GAN(validity, target_labels)\n",
    "        \n",
    "        # Pixel-wise loss\n",
    "        pixel_loss = criterion_L1(fake_images, real_images)\n",
    "        \n",
    "        # Total generator loss\n",
    "        generator_loss = adversarial_loss + 100 * pixel_loss\n",
    "        \n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch+1, num_epochs, i+1, len(dataloader), discriminator_loss.item(), generator_loss.item())\n",
    "        )\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            save_image(fake_images, \"images/%d_%d.png\" % (epoch+1, i+1), normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(torch.cat((real_images, fake_images.detach()), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((real_images, fake_images.detach()),1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
