{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# # Small SIDD dataset\n",
    "# !kaggle datasets download -d rajat95gupta/smartphone-image-denoising-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip 'smartphone-image-denoising-dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataload\n",
    "# augment\n",
    "# visualize\n",
    "# train\n",
    "# save model\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "n_epochs = 200\n",
    "dataset_name = \"SIDD_Small_sRGB_Only\"\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "decay_epoch = 100\n",
    "n_cpu = 8\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "channels = 3\n",
    "sample_interval = 500\n",
    "checkpoint_interval = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "REBUILD_DATA=True\n",
    "\n",
    "class Denoising():\n",
    "    IMG_SIZE=256 \n",
    "    label_dir='SIDD_Small_sRGB_Only/Data/' #dataset directory\n",
    "    training_data=[]\n",
    "    validation_data=[]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_images, self.val_images = train_test_split(os.listdir(self.label_dir), test_size=0.2, random_state=42)\n",
    "\n",
    "    def make_training_and_validation_data(self):\n",
    "        print(self.train_images)\n",
    "        print(self.val_images)\n",
    "\n",
    "        \n",
    "        for image_folder in tqdm(os.listdir(self.label_dir)):\n",
    "            # print(image_folder)\n",
    "            for img_path in glob(self.label_dir + image_folder + '/*'):\n",
    "                try:\n",
    "                    img=cv2.imread(img_path)\n",
    "                    img=cv2.resize(img,(self.IMG_SIZE,self.IMG_SIZE))\n",
    "                    \n",
    "                    # Check if the image belongs to train or validation set\n",
    "                    if image_folder in self.train_images:\n",
    "                        self.training_data.append(np.array(img))\n",
    "                    elif image_folder in self.val_images:\n",
    "                        self.validation_data.append(np.array(img))\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "        np.save('SIDD_Small_sRGB_Only_training_data.npy',self.training_data)\n",
    "        np.save('SIDD_Small_sRGB_Only_validation_data.npy',self.validation_data)\n",
    "        \n",
    "if (REBUILD_DATA):\n",
    "    denoising=Denoising()\n",
    "    denoising.make_training_and_validation_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pickle file\n",
    "training_data=np.load('SIDD_Small_sRGB_Only_training_data.npy',allow_pickle=True)\n",
    "val_data=np.load('SIDD_Small_sRGB_Only_validation_data.npy',allow_pickle=True)\n",
    "len(training_data), len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(data, vis_num):\n",
    "    num_cols = 2\n",
    "    num_rows = vis_num\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            index = i * num_cols + j\n",
    "            axs[i, j].imshow(data[index])\n",
    "            axs[i, j].set_title('Original' if index % 2 == 0 else 'Noisy')\n",
    "            axs[i, j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Assuming you have already loaded training_data and val_data\n",
    "vis_num = 2\n",
    "display_images(training_data, vis_num)\n",
    "display_images(val_data, vis_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import CustomDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = CustomDataset(['SIDD_Small_sRGB_Only_training_data.npy'])\n",
    "# Define your DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = CustomDataset(['SIDD_Small_sRGB_Only_validation_data.npy'])\n",
    "# Define your DataLoader\n",
    "batch_size = 32\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import *\n",
    "from datasets import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image paths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 5003.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Splitting dataset into train and test sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DataLoader to .npy file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DataLoader to .npy file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 661.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from DataLoaderManager import DataLoaderManager\n",
    "from torchvision import transforms\n",
    "import time\n",
    "\n",
    "# Example usage:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image\n",
    "    transforms.ToTensor(),           # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "data_loader_manager = DataLoaderManager(root_dir='SIDD_Small_sRGB_Only/SIDD_Small_sRGB_Only/Data/', transform=transform)\n",
    "dataloader, test_dataloader = data_loader_manager.process_dataloaders(batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "(tensor([[[0.5536, 0.5878, 0.6392,  ..., 0.6221, 0.5536, 0.5707],\n",
      "         [0.6221, 0.6906, 0.7077,  ..., 0.6392, 0.4851, 0.6221],\n",
      "         [0.6392, 0.6734, 0.6392,  ..., 0.7248, 0.5878, 0.6221],\n",
      "         ...,\n",
      "         [1.0673, 1.1358, 1.1872,  ..., 1.0502, 1.0159, 0.9988],\n",
      "         [1.2557, 1.2557, 1.3242,  ..., 1.1872, 1.1872, 1.0673],\n",
      "         [1.1529, 1.1358, 1.3242,  ..., 1.2214, 1.1700, 1.1358]],\n",
      "\n",
      "        [[0.5903, 0.6254, 0.6254,  ..., 0.5378, 0.5203, 0.5378],\n",
      "         [0.6604, 0.7129, 0.6779,  ..., 0.5378, 0.4503, 0.5903],\n",
      "         [0.6429, 0.6779, 0.6254,  ..., 0.6429, 0.5378, 0.5728],\n",
      "         ...,\n",
      "         [1.0280, 1.0630, 1.1155,  ..., 0.9755, 0.9755, 0.9580],\n",
      "         [1.1681, 1.1506, 1.2031,  ..., 1.1155, 1.1155, 1.0105],\n",
      "         [1.0980, 1.0630, 1.2031,  ..., 1.1155, 1.0980, 1.0980]],\n",
      "\n",
      "        [[0.5659, 0.6356, 0.7054,  ..., 0.6008, 0.5485, 0.5834],\n",
      "         [0.6531, 0.7228, 0.7925,  ..., 0.6879, 0.5834, 0.6705],\n",
      "         [0.6356, 0.7054, 0.7402,  ..., 0.8099, 0.7054, 0.7228],\n",
      "         ...,\n",
      "         [0.9842, 1.0714, 1.1237,  ..., 0.9842, 1.0017, 0.9842],\n",
      "         [1.1585, 1.1585, 1.1934,  ..., 1.1062, 1.1237, 1.0191],\n",
      "         [1.1585, 1.1237, 1.1934,  ..., 1.1237, 1.1062, 1.0714]]]), tensor([[[0.5536, 0.5878, 0.6392,  ..., 0.6221, 0.5536, 0.5707],\n",
      "         [0.6221, 0.6906, 0.7077,  ..., 0.6392, 0.4851, 0.6221],\n",
      "         [0.6392, 0.6734, 0.6392,  ..., 0.7248, 0.5878, 0.6221],\n",
      "         ...,\n",
      "         [1.0673, 1.1358, 1.1872,  ..., 1.0502, 1.0159, 0.9988],\n",
      "         [1.2557, 1.2557, 1.3242,  ..., 1.1872, 1.1872, 1.0673],\n",
      "         [1.1529, 1.1358, 1.3242,  ..., 1.2214, 1.1700, 1.1358]],\n",
      "\n",
      "        [[0.5903, 0.6254, 0.6254,  ..., 0.5378, 0.5203, 0.5378],\n",
      "         [0.6604, 0.7129, 0.6779,  ..., 0.5378, 0.4503, 0.5903],\n",
      "         [0.6429, 0.6779, 0.6254,  ..., 0.6429, 0.5378, 0.5728],\n",
      "         ...,\n",
      "         [1.0280, 1.0630, 1.1155,  ..., 0.9755, 0.9755, 0.9580],\n",
      "         [1.1681, 1.1506, 1.2031,  ..., 1.1155, 1.1155, 1.0105],\n",
      "         [1.0980, 1.0630, 1.2031,  ..., 1.1155, 1.0980, 1.0980]],\n",
      "\n",
      "        [[0.5659, 0.6356, 0.7054,  ..., 0.6008, 0.5485, 0.5834],\n",
      "         [0.6531, 0.7228, 0.7925,  ..., 0.6879, 0.5834, 0.6705],\n",
      "         [0.6356, 0.7054, 0.7402,  ..., 0.8099, 0.7054, 0.7228],\n",
      "         ...,\n",
      "         [0.9842, 1.0714, 1.1237,  ..., 0.9842, 1.0017, 0.9842],\n",
      "         [1.1585, 1.1585, 1.1934,  ..., 1.1062, 1.1237, 1.0191],\n",
      "         [1.1585, 1.1237, 1.1934,  ..., 1.1237, 1.1062, 1.0714]]]))\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader.dataset))\n",
    "print(dataloader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def save_dataloader_to_npy(self, dataloader, filename):\n",
    "# print(\"Saving DataLoader to .npy file...\")\n",
    "# data = []\n",
    "# import torch\n",
    "# for batch in tqdm(dataloader):\n",
    "#     gt_images, noisy_images = batch\n",
    "#     for i,img in enumerate(gt_images):\n",
    "#         gt_images = gt_images.numpy()\n",
    "#         noisy_images = noisy_images.numpy()\n",
    "#         data.append([gt_images[i], noisy_images[i]])\n",
    "#     # gt_images = torch.tensor(gt_images) #gt_images.numpy()\n",
    "#     # noisy_images = torch.tensor(noisy_images) #noisy_images.numpy()\n",
    "#     # print(gt_images.shape)\n",
    "#     # # gt_images = gt_images.numpy()\n",
    "#     # # noisy_images = noisy_images.numpy()\n",
    "#     # data.append([gt_images, noisy_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataLoader from .npy file...\n",
      "Loading DataLoader from .npy file...\n",
      "DataLoader loaded from .npy files.\n"
     ]
    }
   ],
   "source": [
    "loaded_dataloader, loaded_test_dataloader = data_loader_manager.process_dataloaders(batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1cdd175eac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[[[[-0.8335474  -0.7650484  -0.8506721  ... -0.7136741  -0.7136741\n",
      "    -0.88492167]\n",
      "   [-0.6280504  -0.6280504  -0.6280504  ... -0.6451751  -0.67942464\n",
      "    -0.7479236 ]\n",
      "   [-0.6109256  -0.6280504  -0.6109256  ... -0.69654936 -0.69654936\n",
      "    -0.69654936]\n",
      "   ...\n",
      "   [-0.5253019  -0.5081771  -0.4054286  ... -0.4054286  -0.42255333\n",
      "    -0.4739276 ]\n",
      "   [-0.5081771  -0.5424266  -0.45680285 ... -0.2855553  -0.33692956\n",
      "    -0.4054286 ]\n",
      "   [-0.5253019  -0.59380084 -0.49105233 ... -0.30268008 -0.33692956\n",
      "    -0.33692956]]\n",
      "\n",
      "  [[-0.687675   -0.617647   -0.687675   ... -0.635154   -0.65266097\n",
      "    -0.792717  ]\n",
      "   [-0.547619   -0.53011197 -0.547619   ... -0.60014    -0.617647\n",
      "    -0.65266097]\n",
      "   [-0.547619   -0.547619   -0.547619   ... -0.635154   -0.635154\n",
      "    -0.617647  ]\n",
      "   ...\n",
      "   [-0.42507    -0.40756297 -0.33753496 ... -0.33753496 -0.35504198\n",
      "    -0.37254897]\n",
      "   [-0.40756297 -0.42507    -0.37254897 ... -0.24999997 -0.30252096\n",
      "    -0.32002798]\n",
      "   [-0.42507    -0.47759098 -0.40756297 ... -0.26750696 -0.28501397\n",
      "    -0.28501397]]\n",
      "\n",
      "  [[-0.5321132  -0.5321132  -0.6541175  ... -0.32296288 -0.30553368\n",
      "    -0.4623964 ]\n",
      "   [-0.2706753  -0.34039208 -0.34039208 ... -0.23581691 -0.23581691\n",
      "    -0.32296288]\n",
      "   [-0.21838771 -0.30553368 -0.30553368 ... -0.23581691 -0.23581691\n",
      "    -0.2706753 ]\n",
      "   ...\n",
      "   [-0.09638336 -0.06152498 -0.02666659 ... -0.0092374  -0.0092374\n",
      "    -0.09638336]\n",
      "   [-0.06152498 -0.09638336 -0.06152498 ...  0.07790857  0.02562099\n",
      "    -0.06152498]\n",
      "   [-0.09638336 -0.14867094 -0.09638336 ...  0.07790857  0.04305018\n",
      "     0.0081918 ]]]\n",
      "\n",
      "\n",
      " [[[-0.8335474  -0.7650484  -0.8506721  ... -0.7136741  -0.7136741\n",
      "    -0.88492167]\n",
      "   [-0.6280504  -0.6280504  -0.6280504  ... -0.6451751  -0.67942464\n",
      "    -0.7479236 ]\n",
      "   [-0.6109256  -0.6280504  -0.6109256  ... -0.69654936 -0.69654936\n",
      "    -0.69654936]\n",
      "   ...\n",
      "   [-0.5253019  -0.5081771  -0.4054286  ... -0.4054286  -0.42255333\n",
      "    -0.4739276 ]\n",
      "   [-0.5081771  -0.5424266  -0.45680285 ... -0.2855553  -0.33692956\n",
      "    -0.4054286 ]\n",
      "   [-0.5253019  -0.59380084 -0.49105233 ... -0.30268008 -0.33692956\n",
      "    -0.33692956]]\n",
      "\n",
      "  [[-0.687675   -0.617647   -0.687675   ... -0.635154   -0.65266097\n",
      "    -0.792717  ]\n",
      "   [-0.547619   -0.53011197 -0.547619   ... -0.60014    -0.617647\n",
      "    -0.65266097]\n",
      "   [-0.547619   -0.547619   -0.547619   ... -0.635154   -0.635154\n",
      "    -0.617647  ]\n",
      "   ...\n",
      "   [-0.42507    -0.40756297 -0.33753496 ... -0.33753496 -0.35504198\n",
      "    -0.37254897]\n",
      "   [-0.40756297 -0.42507    -0.37254897 ... -0.24999997 -0.30252096\n",
      "    -0.32002798]\n",
      "   [-0.42507    -0.47759098 -0.40756297 ... -0.26750696 -0.28501397\n",
      "    -0.28501397]]\n",
      "\n",
      "  [[-0.5321132  -0.5321132  -0.6541175  ... -0.32296288 -0.30553368\n",
      "    -0.4623964 ]\n",
      "   [-0.2706753  -0.34039208 -0.34039208 ... -0.23581691 -0.23581691\n",
      "    -0.32296288]\n",
      "   [-0.21838771 -0.30553368 -0.30553368 ... -0.23581691 -0.23581691\n",
      "    -0.2706753 ]\n",
      "   ...\n",
      "   [-0.09638336 -0.06152498 -0.02666659 ... -0.0092374  -0.0092374\n",
      "    -0.09638336]\n",
      "   [-0.06152498 -0.09638336 -0.06152498 ...  0.07790857  0.02562099\n",
      "    -0.06152498]\n",
      "   [-0.09638336 -0.14867094 -0.09638336 ...  0.07790857  0.04305018\n",
      "     0.0081918 ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(len(loaded_dataloader.dataset))\n",
    "print(loaded_dataloader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_time = time.time()\n",
    "import torch\n",
    "\n",
    "for epoch in range(0, 2 ):\n",
    "    for i, batch in enumerate(loaded_dataloader):\n",
    "        print('fwed')\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[0])\n",
    "        real_B = Variable(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to save dataloader to .npy file\n",
    "def save_dataloader_to_npy(dataloader, filename):\n",
    "    data = []\n",
    "    for batch in dataloader:\n",
    "        gt_images, noisy_images = batch\n",
    "        gt_images = gt_images.numpy()\n",
    "        noisy_images = noisy_images.numpy()\n",
    "        data.append((gt_images, noisy_images))\n",
    "    np.save(filename, data)\n",
    "\n",
    "# Save train dataloader\n",
    "save_dataloader_to_npy(train_dataloader, 'train_dataloader.npy')\n",
    "\n",
    "# Save test dataloader\n",
    "save_dataloader_to_npy(test_dataloader, 'test_dataloader.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(epoch, 2 ):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[0])\n",
    "        real_B = Variable(batch[1])\n",
    "\n",
    "        # # Adversarial ground truths\n",
    "        # valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
    "        # fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # # ------------------\n",
    "        # #  Train Generators\n",
    "        # # ------------------\n",
    "\n",
    "        # optimizer_G.zero_grad()\n",
    "\n",
    "        # # GAN loss\n",
    "        # fake_B = generator(real_A)\n",
    "        # pred_fake = discriminator(fake_B, real_A)\n",
    "        # loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        # # Pixel-wise loss\n",
    "        # loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # # Total loss\n",
    "        # loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        # loss_G.backward()\n",
    "\n",
    "        # optimizer_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        # Iterate through folders and collect image paths\n",
    "        for folder_name in tqdm(os.listdir(root_dir)):\n",
    "            folder_path = os.path.join(root_dir, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for image_name in os.listdir(os.path.join(folder_path)):\n",
    "                    self.image_paths.append({\n",
    "                        'gt': os.path.join(folder_path, image_name),\n",
    "                        'noisy': os.path.join(folder_path, image_name)\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gt_image_path = self.image_paths[idx]['gt']\n",
    "        noisy_image_path = self.image_paths[idx]['noisy']\n",
    "        \n",
    "        gt_image = Image.open(gt_image_path).convert('RGB')\n",
    "        noisy_image = Image.open(noisy_image_path).convert('RGB')\n",
    "\n",
    "        # if self.transform:\n",
    "        #     gt_image = self.transform(gt_image)\n",
    "        #     noisy_image = self.transform(noisy_image)\n",
    "\n",
    "        return gt_image, noisy_image\n",
    "    \n",
    "\n",
    "# Define transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image\n",
    "    transforms.ToTensor(),           # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Set root directory\n",
    "# root_dir = 'data'\n",
    "root_dir='SIDD_Small_sRGB_Only/SIDD_Small_sRGB_Only/Data/' #dataset directory\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(root_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataloaders for train and test sets\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_pixelwise.cuda()\n",
    "\n",
    "if epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (dataset_name, epoch)))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/%s/discriminator_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1,b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "\n",
    "\n",
    "from DataLoader import CustomDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image\n",
    "    transforms.ToTensor(),           # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Set root directory\n",
    "# root_dir = 'data'\n",
    "root_dir='SIDD_Small_sRGB_Only/Data/' #dataset directory\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(root_dir, transform=transform)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataloaders for train and test sets\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Tensor type\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A = Variable(batch[0].type(Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_B = generator(real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(epoch, 2 ):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[0].type(Tensor))\n",
    "        real_B = Variable(batch[1].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B, real_A)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python pix2pix.py --train_dataset_loc 'SIDD_Small_sRGB_Only_training_data.npy' --test_dataset_loc 'SIDD_Small_sRGB_Only_validation_data.npy' --n_epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define loss functions\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to the device\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "lr = 0.0002\n",
    "size = 256\n",
    "\n",
    "# Define dataset and dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.CenterCrop(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# dataset = ImageFolder(root='dataset_path', transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, data in enumerate(dataloader):\n",
    "        print(i)\n",
    "        real_images, _ = data\n",
    "        real_images = real_images.to(device)\n",
    "        \n",
    "        # Train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        fake_images = generator(real_images)\n",
    "        \n",
    "        # Train discriminator with real images\n",
    "        real_validity = discriminator(torch.cat((real_images, real_images), 1))\n",
    "        real_labels = torch.ones(real_validity.size()).to(device)\n",
    "        real_loss = criterion_GAN(real_validity, real_labels)\n",
    "        \n",
    "        # Train discriminator with fake images\n",
    "        fake_validity = discriminator(torch.cat((real_images, fake_images.detach()), 1))\n",
    "        fake_labels = torch.zeros(fake_validity.size()).to(device)\n",
    "        fake_loss = criterion_GAN(fake_validity, fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        discriminator_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Adversarial loss\n",
    "        validity = discriminator(torch.cat((real_images, fake_images), 1))\n",
    "        target_labels = torch.ones(validity.size()).to(device)\n",
    "        adversarial_loss = criterion_GAN(validity, target_labels)\n",
    "        \n",
    "        # Pixel-wise loss\n",
    "        pixel_loss = criterion_L1(fake_images, real_images)\n",
    "        \n",
    "        # Total generator loss\n",
    "        generator_loss = adversarial_loss + 100 * pixel_loss\n",
    "        \n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch+1, num_epochs, i+1, len(dataloader), discriminator_loss.item(), generator_loss.item())\n",
    "        )\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            save_image(fake_images, \"images/%d_%d.png\" % (epoch+1, i+1), normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator(torch.cat((real_images, fake_images.detach()), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((real_images, fake_images.detach()),1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
