{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
    "# !tar -xvf facades.tar.gz\n",
    "\n",
    "# https://github.com/aniketmaurya/pytorch-gans/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./facades/train/\"\n",
    "\n",
    "\n",
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, path, target_size=None):\n",
    "        self.filenames = glob(str(Path(path) / \"*\"))\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        image = Image.open(filename)\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image_width = image.shape[2]\n",
    "\n",
    "        real = image[:, :, : image_width // 2]\n",
    "        condition = image[:, :, image_width // 2 :]\n",
    "\n",
    "        target_size = self.target_size\n",
    "        if target_size:\n",
    "            condition = nn.functional.interpolate(condition, size=target_size)\n",
    "            real = nn.functional.interpolate(real, size=target_size)\n",
    "\n",
    "        return real, condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSampleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True):\n",
    "        \"\"\"\n",
    "        Paper details:\n",
    "        - C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        - All convolutions are 4×4 spatial filters applied with stride 2\n",
    "        - Convolutions in the encoder downsample by a factor of 2\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "        if self.activation:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleConv(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel=4,\n",
    "        strides=2,\n",
    "        padding=1,\n",
    "        activation=True,\n",
    "        batchnorm=True,\n",
    "        dropout=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.ReLU(True)\n",
    "\n",
    "        if dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Paper details:\n",
    "        - Encoder: C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        - All convolutions are 4×4 spatial filters applied with stride 2\n",
    "        - Convolutions in the encoder downsample by a factor of 2\n",
    "        - Decoder: CD512-CD1024-CD1024-C1024-C1024-C512 -C256-C128\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder/donwsample convs\n",
    "        self.encoders = [\n",
    "            DownSampleConv(in_channels, 64, batchnorm=False),  # bs x 64 x 128 x 128\n",
    "            DownSampleConv(64, 128),  # bs x 128 x 64 x 64\n",
    "            DownSampleConv(128, 256),  # bs x 256 x 32 x 32\n",
    "            DownSampleConv(256, 512),  # bs x 512 x 16 x 16\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 8 x 8\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 4 x 4\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 2 x 2\n",
    "            DownSampleConv(512, 512, batchnorm=False),  # bs x 512 x 1 x 1\n",
    "        ]\n",
    "\n",
    "        # decoder/upsample convs\n",
    "        self.decoders = [\n",
    "            UpSampleConv(512, 512, dropout=True),  # bs x 512 x 2 x 2\n",
    "            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 4 x 4\n",
    "            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 8 x 8\n",
    "            UpSampleConv(1024, 512),  # bs x 512 x 16 x 16\n",
    "            UpSampleConv(1024, 256),  # bs x 256 x 32 x 32\n",
    "            UpSampleConv(512, 128),  # bs x 128 x 64 x 64\n",
    "            UpSampleConv(256, 64),  # bs x 64 x 128 x 128\n",
    "        ]\n",
    "        self.decoder_channels = [512, 512, 512, 512, 256, 128, 64]\n",
    "        self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips_cons = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "\n",
    "            skips_cons.append(x)\n",
    "\n",
    "        skips_cons = list(reversed(skips_cons[:-1]))\n",
    "        decoders = self.decoders[:-1]\n",
    "\n",
    "        for decoder, skip in zip(decoders, skips_cons):\n",
    "            x = decoder(x)\n",
    "            # print(x.shape, skip.shape)\n",
    "            x = torch.cat((x, skip), axis=1)\n",
    "\n",
    "        x = self.decoders[-1](x)\n",
    "        # print(x.shape)\n",
    "        x = self.final_conv(x)\n",
    "        return self.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.d1 = DownSampleConv(input_channels, 64, batchnorm=False)\n",
    "        self.d2 = DownSampleConv(64, 128)\n",
    "        self.d3 = DownSampleConv(128, 256)\n",
    "        self.d4 = DownSampleConv(256, 512)\n",
    "        self.final = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], axis=1)\n",
    "        x0 = self.d1(x)\n",
    "        x1 = self.d2(x0)\n",
    "        x2 = self.d3(x1)\n",
    "        x3 = self.d4(x2)\n",
    "        xn = self.final(x3)\n",
    "        return xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "reconstruction_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt  # Import for using plt\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "def display_progress(cond, real, fake, figsize=(10,5)):\n",
    "    cond = cond.detach().cpu().permute(1, 2, 0)\n",
    "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
    "    real = real.detach().cpu().permute(1, 2, 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    ax[0].imshow(cond)\n",
    "    ax[1].imshow(real)\n",
    "    ax[2].imshow(fake)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=200, display_step=25):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.display_step = display_step\n",
    "        self.gen = Generator(in_channels, out_channels)\n",
    "        self.patch_gan = PatchGAN(in_channels + out_channels)\n",
    "\n",
    "        # intializing weights\n",
    "        self.gen = self.gen.apply(_weights_init)\n",
    "        self.patch_gan = self.patch_gan.apply(_weights_init)\n",
    "\n",
    "        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "\n",
    "    def _gen_step(self, real_images, conditioned_images):\n",
    "        # Pix2Pix has adversarial and a reconstruction loss\n",
    "        # First calculate the adversarial loss\n",
    "        fake_images = self.gen(conditioned_images)\n",
    "        disc_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "        adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits))\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        lambda_recon = self.hparams.lambda_recon\n",
    "\n",
    "        return adversarial_loss + lambda_recon * recon_loss\n",
    "\n",
    "    def _disc_step(self, real_images, conditioned_images):\n",
    "        fake_images = self.gen(conditioned_images).detach()\n",
    "        fake_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "\n",
    "        real_logits = self.patch_gan(real_images, conditioned_images)\n",
    "\n",
    "        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n",
    "        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n",
    "        return (real_loss + fake_loss) / 2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.learning_rate\n",
    "        gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr)\n",
    "        disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr)\n",
    "        return disc_opt, gen_opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        real, condition = batch\n",
    "\n",
    "        # loss = None\n",
    "        # if optimizer_idx == 0:\n",
    "        #     loss = self._disc_step(real, condition)\n",
    "        #     self.log('PatchGAN Loss', loss)\n",
    "        # elif optimizer_idx == 1:\n",
    "        #     loss = self._gen_step(real, condition)\n",
    "        #     self.log('Generator Loss', loss)\n",
    "\n",
    "\n",
    "        if batch_idx % 2 == 0:  # Train discriminator on even batches\n",
    "            loss = self._disc_step(real, condition)\n",
    "            gen_opt, disc_opt = self.optimizers()  # Access optimizers\n",
    "            disc_opt.zero_grad()  # Zero gradients for discriminator\n",
    "            loss.backward()\n",
    "            disc_opt.step()  # Update discriminator parameters\n",
    "            self.log('PatchGAN Loss', loss)\n",
    "        else:  # Train generator on odd batches\n",
    "            loss = self._gen_step(real, condition)\n",
    "            gen_opt, disc_opt = self.optimizers()  # Access optimizers\n",
    "            gen_opt.zero_grad()  # Zero gradients for generator\n",
    "            loss.backward()\n",
    "            gen_opt.step()  # Update generator parameters\n",
    "            self.log('Generator Loss', loss)\n",
    "        \n",
    "        if self.current_epoch%self.display_step==0 and batch_idx==0:\n",
    "            fake = self.gen(condition).detach()\n",
    "            display_progress(condition[0], fake[0], real[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=200, display_step=25):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization = False\n",
    "        self.display_step = display_step\n",
    "        self.gen = Generator(in_channels, out_channels)\n",
    "        self.patch_gan = PatchGAN(in_channels + out_channels)\n",
    "\n",
    "        # intializing weights\n",
    "        self.gen = self.gen.apply(_weights_init)\n",
    "        self.patch_gan = self.patch_gan.apply(_weights_init)\n",
    "\n",
    "        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "\n",
    "    def _gen_step(self, real_images, conditioned_images):\n",
    "        # Pix2Pix has adversarial and a reconstruction loss\n",
    "        # First calculate the adversarial loss\n",
    "        fake_images = self.gen(conditioned_images)\n",
    "        disc_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "        adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits))\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        lambda_recon = self.hparams.lambda_recon\n",
    "\n",
    "        return adversarial_loss + lambda_recon * recon_loss\n",
    "\n",
    "    def _disc_step(self, real_images, conditioned_images):\n",
    "        fake_images = self.gen(conditioned_images).detach()\n",
    "        fake_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "\n",
    "        real_logits = self.patch_gan(real_images, conditioned_images)\n",
    "\n",
    "        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n",
    "        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n",
    "        return (real_loss + fake_loss) / 2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.learning_rate\n",
    "        gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr)\n",
    "        disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr)\n",
    "        return disc_opt, gen_opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        real, condition = batch\n",
    "\n",
    "        loss = None\n",
    "        if batch_idx % 2 == 0:  # Train discriminator on even batches\n",
    "            loss = self._disc_step(real, condition)\n",
    "            gen_opt, disc_opt = self.optimizers()  # Access optimizers\n",
    "            disc_opt.zero_grad()  # Zero gradients for discriminator\n",
    "            loss.backward()\n",
    "            disc_opt.step()  # Update discriminator parameters\n",
    "            self.log('PatchGAN Loss', loss)\n",
    "            print('PatchGAN Loss', loss)\n",
    "\n",
    "        else:  # Train generator on odd batches\n",
    "            loss = self._gen_step(real, condition)\n",
    "            gen_opt, disc_opt = self.optimizers()  # Access optimizers\n",
    "            gen_opt.zero_grad()  # Zero gradients for generator\n",
    "            loss.backward()\n",
    "            gen_opt.step()  # Update generator parameters\n",
    "            self.log('Generator Loss', loss)\n",
    "            print('Generator Loss', loss)\n",
    "\n",
    "        # if self.current_epoch%self.display_step==0 and batch_idx==0:\n",
    "        #     fake = self.gen(condition).detach()\n",
    "        #     display_progress(condition[0], real[0], fake[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchGAN Loss tensor(0.7712, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Generator Loss tensor(98.6618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "PatchGAN Loss "
     ]
    }
   ],
   "source": [
    "target_size = None\n",
    "lr=0.0002\n",
    "lambda_recon=200\n",
    "display_step=25\n",
    "batch_size = 128\n",
    "\n",
    "display_step = True\n",
    "dataset = FacadesDataset(path, target_size=target_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "pix2pix = Pix2Pix(3, 3, learning_rate=lr, lambda_recon=lambda_recon, display_step=display_step)\n",
    "trainer = pl.Trainer(max_epochs=2000)\n",
    "trainer.fit(pix2pix, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
