{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/Adi-iitd/AI-Art/blob/master/src/Pix2Pix/Imports.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os, wget, zipfile, shutil\n",
    "import warnings, itertools, functools\n",
    "\n",
    "from collections import OrderedDict\n",
    "from argparse import ArgumentParser\n",
    "from skimage import io as io, transform as tfm\n",
    "\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "mpl.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "mpl.rcParams[\"axes.grid\"     ] = False\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Conv2d as Conv, ConvTranspose2d as Deconv,  ReLU as Relu\n",
    "from torch.nn import InstanceNorm2d as InstanceNorm, BatchNorm2d as BatchNorm\n",
    "# from torch.utils.tensorboard import SummaryWriter,  FileWriter,  RecordWriter\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, Callback, ModelCheckpoint\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset facades!!\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 300\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39m# You can decrease the num_workers argument in {train/val/test}_dataloader\u001b[39;00m\n\u001b[0;32m    299\u001b[0m datamodule \u001b[39m=\u001b[39m DataModule(url, root_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./Dataset/Pix2Pix/\u001b[39m\u001b[39m\"\u001b[39m, trn_batch_sz \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, tst_batch_sz \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m)\n\u001b[1;32m--> 300\u001b[0m datamodule\u001b[39m.\u001b[39;49mprepare_data()\n\u001b[0;32m    301\u001b[0m datamodule\u001b[39m.\u001b[39msetup(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFew random samples from the Training dataset!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 218\u001b[0m, in \u001b[0;36mDataModule.prepare_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading dataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m!!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 218\u001b[0m     wget\u001b[39m.\u001b[39;49mdownload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompressed_dir)\n\u001b[0;32m    219\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mDataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m downloaded. Extraction in progress!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    221\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompressed_dir \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m zip_ref:\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\site-packages\\wget.py:526\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, out, bar)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     binurl \u001b[39m=\u001b[39m url\n\u001b[1;32m--> 526\u001b[0m (tmpfile, headers) \u001b[39m=\u001b[39m ulib\u001b[39m.\u001b[39;49murlretrieve(binurl, tmpfile, callback)\n\u001b[0;32m    527\u001b[0m filename \u001b[39m=\u001b[39m detect_filename(url, out, headers)\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m outdir:\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\urllib\\request.py:239\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m url_type, path \u001b[39m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 239\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mclosing(urlopen(url, data)) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    240\u001b[0m     headers \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39minfo()\n\u001b[0;32m    242\u001b[0m     \u001b[39m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[39mfor\u001b[39;00m processor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response\u001b[39m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[0;32m    525\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[39m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[39m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[0;32m    633\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[0;32m    635\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rs659\\AppData\\Local\\anaconda3\\envs\\imgdenv\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, image_size: (int, tuple) = 256):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            image_size: Final size of the image\n",
    "        \"\"\"\n",
    "\n",
    "        if   isinstance(image_size, int):   self.image_size = (image_size, image_size)\n",
    "        elif isinstance(image_size, tuple): self.image_size = image_size\n",
    "        else: raise ValueError(\"Unknown DataType of the parameter image_size found!!\")\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "\n",
    "        A = tfm.resize(A, output_shape = self.image_size)\n",
    "        B = tfm.resize(B, output_shape = self.image_size)\n",
    "\n",
    "        A = np.clip(A, a_min = 0., a_max = 1.)\n",
    "        B = np.clip(B, a_min = 0., a_max = 1.)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "\n",
    "    def __init__(self, image_size: (int, tuple) = 256):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            image_size: Final size of the image (should be smaller than current size o/w returns the original image)\n",
    "        \"\"\"\n",
    "\n",
    "        if   isinstance(image_size, int):   self.image_size = (image_size, image_size)\n",
    "        elif isinstance(image_size, tuple): self.image_size = image_size\n",
    "        else: raise ValueError(\"Unknown DataType of the parameter image_size found!!\")\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "        curr_height, curr_width = A.shape[0], A.shape[1]\n",
    "\n",
    "        ht_diff = max(0, curr_height - self.image_size[0])\n",
    "        wd_diff = max(0, curr_width  - self.image_size[1])\n",
    "        top = np.random.randint(low = 0, high = ht_diff)\n",
    "        lft = np.random.randint(low = 0, high = wd_diff)\n",
    "\n",
    "        A = A[top: top + self.image_size[0], lft: lft + self.image_size[1]]\n",
    "        B = B[top: top + self.image_size[0], lft: lft + self.image_size[1]]\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class Random_Flip(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "        if np.random.uniform(low = 0., high = 1.0) > .5:\n",
    "            A = np.fliplr(A)\n",
    "            B = np.fliplr(B)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class To_Tensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A = np.transpose(sample['A'].astype(np.float, copy = True), (2, 0, 1))\n",
    "        B = np.transpose(sample['B'].astype(np.float, copy = True), (2, 0, 1))\n",
    "\n",
    "        A = torch.tensor(A, dtype = torch.float)\n",
    "        B = torch.tensor(B, dtype = torch.float)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "\n",
    "    def __init__(self, mean = [0.5] * 3, stdv = [0.5] * 3):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            mean: Normalizing mean\n",
    "            stdv: Normalizing stdv\n",
    "        \"\"\"\n",
    "\n",
    "        mean = torch.tensor(mean, dtype = torch.float)\n",
    "        stdv = torch.tensor(stdv, dtype = torch.float)\n",
    "        self.transforms = T.Normalize(mean = mean, std = stdv)\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sample: Dictionary containing image and label\n",
    "        \"\"\"\n",
    "\n",
    "        A, B = sample['A'], sample['B']\n",
    "        A = self.transforms(A)\n",
    "        B = self.transforms(B)\n",
    "\n",
    "        return {'A': A, 'B': B}\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path: str = None, transforms = None, max_sz: int = 1000):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            transforms: a list of Transformations (Data augmentation)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(); self.transforms = T.Compose(transforms)\n",
    "\n",
    "        file_names_A = sorted(os.listdir(path + 'A/'), key = lambda x: int(x[: -4]))\n",
    "        self.file_names_A = [path + 'A/' + file_name for file_name in file_names_A]\n",
    "\n",
    "        file_names_B = sorted(os.listdir(path + 'B/'), key = lambda x: int(x[: -4]))\n",
    "        self.file_names_B = [path + 'B/' + file_name for file_name in file_names_B]\n",
    "\n",
    "        self.file_names_A = self.file_names_A[:max_sz]\n",
    "        self.file_names_B = self.file_names_B[:max_sz]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.file_names_A) == len(self.file_names_B)\n",
    "        return len(self.file_names_A)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        A = io.imread(self.file_names_A[idx])\n",
    "        B = io.imread(self.file_names_B[idx])\n",
    "        sample = self.transforms({'A': A, 'B': B})\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    \"\"\"\n",
    "    Download the dataset using the below link; you just need to specify the url while creating an object of this class\n",
    "    https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\n",
    "    Authors don't follow a consistent format for all the datasets, so, it might not work for few\n",
    "\n",
    "    Implements the Lightining DataModule!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url: str, root_dir: str = \"./Dataset/CycleGAN/\", img_sz: int = 256, trn_batch_sz: int = 4,\n",
    "                 tst_batch_sz: int = 64):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            url:          Download URL of the dataset\n",
    "            root_dir:     Root dir where dataset needs to be downloaded\n",
    "            img_sz:       Size of the Image\n",
    "            trn_batch_sz: Training Batch Size\n",
    "            tst_batch_sz: Test Batch Size\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.url = url\n",
    "        self.dataset = url.split(\"/\")[-1]\n",
    "\n",
    "        self.processed_dir  = root_dir + \"Processed/\"\n",
    "        self.compressed_dir = root_dir + \"Compressed/\"\n",
    "        os.makedirs(self.processed_dir , exist_ok = True)\n",
    "        os.makedirs(self.compressed_dir, exist_ok = True)\n",
    "\n",
    "        self.trn_batch_sz = trn_batch_sz\n",
    "        self.tst_batch_sz = tst_batch_sz\n",
    "\n",
    "        jitter_sz = int(img_sz * 1.120)\n",
    "        self.tst_tfms = [Resize(img_sz), To_Tensor(), Normalize()]\n",
    "        self.trn_tfms = [Resize(jitter_sz), RandomCrop(img_sz), Random_Flip(), To_Tensor(), Normalize()]\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        if self.dataset in os.listdir(self.compressed_dir):\n",
    "            print(f\"Dataset {self.dataset[:-4]} already exists!\")\n",
    "        else:\n",
    "            print(f\"Downloading dataset {self.dataset[:-4]}!!\")\n",
    "            wget.download(self.url, self.compressed_dir)\n",
    "            print(f\"\\nDataset {self.dataset[:-4]} downloaded. Extraction in progress!\")\n",
    "\n",
    "            with zipfile.ZipFile(self.compressed_dir + self.dataset, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.processed_dir)\n",
    "            print(f\"Extraction done!\")\n",
    "\n",
    "            # you might need to modify the below code; it's not generic, but works for most of the datasets \n",
    "            # listed in that url.\n",
    "            \n",
    "            dwnld_dir = self.processed_dir + self.dataset[:-4] + \"/\"\n",
    "            for folder in [\"testA/\", \"testB/\", \"trainA/\", \"trainB/\"]:\n",
    "\n",
    "                dest_dir = dwnld_dir\n",
    "                src_dir  = dwnld_dir + folder\n",
    "\n",
    "                dest_dir = dest_dir + \"Train/\" if folder[:-2] != \"test\" else dest_dir + \"Test/\"\n",
    "                dest_dir = dest_dir + \"B/\"     if folder[-2]  != \"A\"    else dest_dir + \"A/\"\n",
    "                os.makedirs(dest_dir, exist_ok = True)\n",
    "\n",
    "                orig_files = [src_dir  + file for file in sorted(os.listdir(src_dir))]\n",
    "                modf_files = [dest_dir + \"{:06d}.jpg\".format(i) for i, file in enumerate(orig_files)]\n",
    "\n",
    "                for orig_file, modf_file in zip(orig_files, modf_files):\n",
    "                    shutil.move(orig_file, modf_file)\n",
    "                os.rmdir(src_dir)\n",
    "\n",
    "            print(f\"Files moved to appropiate folder!\")\n",
    "\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        \"\"\"\n",
    "        stage: fit/test\n",
    "        \"\"\"\n",
    "\n",
    "        dwnld_dir = self.processed_dir + self.dataset[:-4]\n",
    "        trn_dir = dwnld_dir + \"/Train/\"\n",
    "        tst_dir = dwnld_dir + \"/Test/\"\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "\n",
    "            dataset = CustomDataset(path = trn_dir, transforms = self.trn_tfms)\n",
    "            train_sz = int(len(dataset) * 0.9)\n",
    "            valid_sz = len(dataset) - train_sz\n",
    "\n",
    "            self.train, self.valid = random_split(dataset, [train_sz, valid_sz])\n",
    "            print(f\"Size of the training dataset: {train_sz}, validation dataset: {valid_sz}\")\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test = CustomDataset(path = tst_dir, transforms = self.tst_tfms)\n",
    "            print(f\"Size of the test dataset: {len(self.test)}\")\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size = self.trn_batch_sz, shuffle = True , num_workers = 16, \n",
    "                          pin_memory = True)\n",
    "\n",
    "    def val_dataloader  (self):\n",
    "        return DataLoader(self.valid, batch_size = self.tst_batch_sz, shuffle = False, num_workers = 16, \n",
    "                          pin_memory = True)\n",
    "\n",
    "    def test_dataloader (self):\n",
    "        return DataLoader(self.test , batch_size = self.tst_batch_sz, shuffle = False, num_workers = 16, \n",
    "                          pin_memory = True)\n",
    "\n",
    "\n",
    "def show_image(image):\n",
    "    plt.imshow(np.transpose((image + 1) / 2, (1, 2, 0)))\n",
    "\n",
    "def get_random_sample(dataset):\n",
    "    return dataset[np.random.randint(0, len(dataset))]\n",
    "\n",
    "\n",
    "###############################################################################################################################################\n",
    "\n",
    "\n",
    "img_sz = 256\n",
    "url = \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/facades.zip\"\n",
    "\n",
    "# You can decrease the num_workers argument in {train/val/test}_dataloader\n",
    "datamodule = DataModule(url, root_dir = \"./Dataset/Pix2Pix/\", trn_batch_sz = 1, tst_batch_sz = 64)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "\n",
    "print(f\"Few random samples from the Training dataset!\")\n",
    "\n",
    "sample = get_random_sample(datamodule.train)\n",
    "plt.subplot(1, 2, 1); show_image(sample['A'])\n",
    "plt.subplot(1, 2, 2); show_image(sample['B'])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Few random samples from the Validation dataset!\")\n",
    "\n",
    "sample = get_random_sample(datamodule.valid)\n",
    "plt.subplot(1, 2, 1); show_image(sample['A'])\n",
    "plt.subplot(1, 2, 2); show_image(sample['B'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###############################################################################################################################################\n",
    "\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels: int, inner_channels: int, innermost: bool = False, outermost: bool = False,\n",
    "                 apply_dp: bool = False, submodule = None, add_skip_conn: bool = True, norm_type: str = 'instance'):\n",
    "\n",
    "\n",
    "        \"\"\"Defines a Unet submodule with/without skip connection!\n",
    "        X -----------------identity(optional)--------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_channels: Number of output channels in the DeConvolutional layer\n",
    "            inner_channels: Number of output channels in the Convolutional layer\n",
    "            innermost:      If this module is the innermost module\n",
    "            outermost:      If this module is the outermost module\n",
    "            apply_dp:       If apply_dp is set to True, then activations are 0'ed out with prob 0.5\n",
    "            submodule:      Previously defined UNet submodule\n",
    "            add_skip_conn:  If set to true, skip connections are added b/w Encoder and Decoder\n",
    "            norm_type:      Type of Normalization layer - InstanceNorm2D or BatchNorm2D\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.outermost = outermost\n",
    "        self.add_skip_conn = add_skip_conn\n",
    "\n",
    "        bias = norm_type == 'instance'\n",
    "        f = 2 if add_skip_conn else 1\n",
    "        norm_layer = InstanceNorm if norm_type == 'instance' else BatchNorm\n",
    "\n",
    "        if  innermost:\n",
    "            dn_conv = Conv  (in_channels = input_channels, out_channels = inner_channels, kernel_size = 4, stride = 2,\n",
    "                             padding = 1, bias = True, padding_mode = 'zeros')\n",
    "            up_conv = Deconv(in_channels = inner_channels, out_channels = input_channels, kernel_size = 4, stride = 2,\n",
    "                             padding = 1, bias = bias, padding_mode = 'zeros')\n",
    "\n",
    "            dn_layers = [nn.LeakyReLU(0.2, True), dn_conv]\n",
    "            up_layers = [nn.ReLU(True), up_conv, norm_layer(input_channels)]\n",
    "            layers = dn_layers + up_layers\n",
    "\n",
    "        elif outermost:\n",
    "            dn_conv = Conv  (in_channels = 1 * input_channels, out_channels = inner_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = True, padding_mode = 'zeros')\n",
    "            up_conv = Deconv(in_channels = f * inner_channels, out_channels = input_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = True, padding_mode = 'zeros')\n",
    "\n",
    "            dn_layers = [dn_conv]\n",
    "            up_layers = [nn.ReLU(True), up_conv, nn.Tanh()]\n",
    "            layers = dn_layers + [submodule] + up_layers\n",
    "\n",
    "        else:\n",
    "            dn_conv = Conv  (in_channels = 1 * input_channels, out_channels = inner_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = bias, padding_mode = 'zeros')\n",
    "            up_conv = Deconv(in_channels = f * inner_channels, out_channels = input_channels, kernel_size = 4,\n",
    "                             stride = 2, padding = 1, bias = bias, padding_mode = 'zeros')\n",
    "\n",
    "            dn_layers = [nn.LeakyReLU(0.2, True), dn_conv, norm_layer(inner_channels)]\n",
    "            up_layers = [nn.ReLU(True), up_conv, norm_layer(input_channels)]\n",
    "\n",
    "            if apply_dp:\n",
    "                layers = dn_layers + [submodule] + up_layers + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                layers = dn_layers + [submodule] + up_layers\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.outermost: return self.net(x)\n",
    "        else: return torch.cat([x, self.net(x)], dim = 1) if self.add_skip_conn else self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, out_channels: int = 64, nb_layers: int = 8, apply_dp: bool = True,\n",
    "                 add_skip_conn: bool = True, norm_type: str = 'instance'):\n",
    "\n",
    "        \"\"\"\n",
    "                            Generator Architecture!\n",
    "        Encoder:        C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        U-Net Decoder:  CD1024-CD1024-CD1024-CD1024-CD512-CD256-CD128, where Ck denote a Convolution-InsNorm-ReLU\n",
    "        layer with k filters, and CDk denotes a Convolution-InsNorm-Dropout-ReLU layer with a dropout rate of 50%\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            in_channels:    Number of input channels\n",
    "            out_channels:   Number of output channels\n",
    "            nb_layers:      Number of layers in the Generator\n",
    "            apply_dp:       If apply_dp is set to True, then activations are 0'ed out with prob \"drop_param\"\n",
    "            add_skip_conn:  If set to true, skip connections are added b/w Encoder and Decoder\n",
    "            norm_type:      Type of Normalization layer - InstanceNorm2D or BatchNorm2D\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        f = 4\n",
    "        self.layers = []\n",
    "\n",
    "        unet = UNetBlock(out_channels * 8, out_channels * 8, innermost = True, outermost = False, apply_dp = False,\n",
    "                         submodule = None, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "\n",
    "        for idx in range(nb_layers - 5):\n",
    "            unet = UNetBlock(out_channels * 8, out_channels * 8, innermost = False, outermost = False, apply_dp =\n",
    "                             apply_dp, submodule = unet, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "\n",
    "        for idx in range(0, 3):\n",
    "            unet = UNetBlock(out_channels * f, out_channels*2*f, innermost = False, outermost = False, apply_dp =\n",
    "                             False,    submodule = unet, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "            f = f // 2\n",
    "\n",
    "        unet = UNetBlock(in_channels * 1, out_channels * 1, innermost = False, outermost = True,  apply_dp = False,\n",
    "                         submodule = unet, add_skip_conn = add_skip_conn, norm_type = norm_type)\n",
    "\n",
    "        self.net = unet\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, nb_layers = 3, norm_type: str = 'instance'):\n",
    "\n",
    "        \"\"\"\n",
    "                                    Discriminator Architecture!\n",
    "        C64 - C128 - C256 - C512, where Ck denote a Convolution-InstanceNorm-LeakyReLU layer with k filters\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            in_channels:    Number of input channels\n",
    "            out_channels:   Number of output channels\n",
    "            nb_layers:      Number of layers in the 70*70 Patch Discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        in_f  = 1\n",
    "        out_f = 2\n",
    "        bias = norm_type == 'instance'\n",
    "        norm_layer = InstanceNorm if norm_type == \"instance\" else BatchNorm\n",
    "\n",
    "        conv = Conv(in_channels, out_channels, 4, stride = 2, padding = 1, bias = True)\n",
    "        layers = [conv, nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        for idx in range(1, nb_layers):\n",
    "            conv = Conv(out_channels * in_f, out_channels * out_f, 4, stride = 2, padding = 1, bias = bias)\n",
    "            layers += [conv, norm_layer(out_channels * out_f), nn.LeakyReLU(0.2, True)]\n",
    "            in_f = out_f; out_f *= 2\n",
    "\n",
    "        out_f = min(2 ** nb_layers, 8)\n",
    "        conv = Conv(out_channels * in_f, out_channels * out_f, 4, stride = 1, padding = 1, bias = bias)\n",
    "        layers += [conv, norm_layer(out_channels * out_f), nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        conv = Conv(out_channels * out_f, 1, 4, stride = 1, padding = 1, bias = True)\n",
    "        layers += [conv]\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Initializer:\n",
    "\n",
    "    def __init__(self, init_type: str = 'normal', init_gain: float = 0.02):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            init_type: Initializer type - 'kaiming' or 'xavier' or 'normal'\n",
    "            init_gain: Standard deviation of the normal distribution\n",
    "        \"\"\"\n",
    "\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "\n",
    "    def init_module(self, m):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            m: Module\n",
    "        \"\"\"\n",
    "\n",
    "        cls_name = m.__class__.__name__;\n",
    "        if hasattr(m, 'weight') and (cls_name.find('Conv') != -1 or cls_name.find('Linear') != -1):\n",
    "\n",
    "            if   self.init_type == 'kaiming': nn.init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
    "            elif self.init_type == 'xavier' : nn.init.xavier_normal_ (m.weight.data,  gain = self.init_gain)\n",
    "            elif self.init_type == 'normal' : nn.init.normal_(m.weight.data, mean = 0, std = self.init_gain)\n",
    "            else: raise ValueError('Initialization not found!!')\n",
    "\n",
    "            if m.bias is not None: nn.init.constant_(m.bias.data, val = 0);\n",
    "\n",
    "        if hasattr(m, 'weight') and cls_name.find('BatchNorm2d') != -1:\n",
    "            nn.init.normal_(m.weight.data, mean = 1.0, std = self.init_gain)\n",
    "            nn.init.constant_(m.bias.data, val = 0)\n",
    "\n",
    "\n",
    "    def __call__(self, net):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            net: Network\n",
    "        \"\"\"\n",
    "        \n",
    "        net.apply(self.init_module)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "\n",
    "class Loss:\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements different losses required to train the generators and discriminators of CycleGAN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_type: str = 'MSE', lambda_: int = 100):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            loss_type: Loss Function to train CycleGAN\n",
    "            lambda_:   Weightage of Cycle-consistency loss\n",
    "        \"\"\"\n",
    "\n",
    "        self.loss = nn.MSELoss() if loss_type == 'MSE' else nn.BCEWithLogitsLoss()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "\n",
    "    def get_dis_loss(self, dis_pred_real_data, dis_pred_fake_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dis_pred_real_data: Discriminator's prediction on real data\n",
    "            dis_pred_fake_data: Discriminator's prediction on fake data\n",
    "        \"\"\"\n",
    "\n",
    "        dis_tar_real_data = torch.ones_like (dis_pred_real_data, requires_grad = False)\n",
    "        dis_tar_fake_data = torch.zeros_like(dis_pred_fake_data, requires_grad = False)\n",
    "\n",
    "        loss_real_data = self.loss(dis_pred_real_data, dis_tar_real_data)\n",
    "        loss_fake_data = self.loss(dis_pred_fake_data, dis_tar_fake_data)\n",
    "\n",
    "        dis_tot_loss = (loss_real_data + loss_fake_data) * 0.5\n",
    "\n",
    "        return dis_tot_loss\n",
    "\n",
    "\n",
    "    def get_gen_gan_loss(self, dis_pred_fake_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dis_pred_fake_data: Discriminator's prediction on fake data\n",
    "        \"\"\"\n",
    "\n",
    "        gen_tar_fake_data = torch.ones_like(dis_pred_fake_data, requires_grad = False)\n",
    "        gen_tot_loss = self.loss(dis_pred_fake_data, gen_tar_fake_data)\n",
    "\n",
    "        return gen_tot_loss\n",
    "\n",
    "\n",
    "    def get_gen_rec_loss(self, real_data, recs_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            real_data: Real images sampled from the dataloaders\n",
    "            recs_data: Fake label generated by the generator\n",
    "        \"\"\"\n",
    "\n",
    "        gen_rec_loss = torch.nn.L1Loss()(real_data, recs_data)\n",
    "        gen_tot_loss = gen_rec_loss * self.lambda_\n",
    "\n",
    "        return gen_tot_loss\n",
    "\n",
    "\n",
    "    def get_gen_loss(self, dis_pred_fake_data, real_data, fake_data):\n",
    "\n",
    "        \"\"\"\n",
    "        Implements the total Generator loss\n",
    "        Sum of Reconstruction loss, and GAN loss\n",
    "        \"\"\"\n",
    "\n",
    "        gen_gan_loss = self.get_gen_gan_loss(dis_pred_fake_data  )\n",
    "        gen_rec_loss = self.get_gen_rec_loss(real_data, fake_data)\n",
    "        gen_tot_loss = gen_gan_loss + gen_rec_loss\n",
    "\n",
    "        return gen_tot_loss\n",
    "\n",
    "\n",
    "\n",
    "class Pix2Pix(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, d_lr: float = 2e-4, g_lr: float = 2e-4, beta_1: float = 0.5, beta_2: float = 0.999, epoch_decay: int = 100):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_lr = d_lr\n",
    "        self.g_lr = g_lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epoch_decay = epoch_decay\n",
    "\n",
    "        self.loss = Loss(loss_type = 'MSE', lambda_ = 100)\n",
    "        init = Initializer(init_type = 'normal', init_gain = 0.02)\n",
    "        \n",
    "        self.gen  = init(Generator(in_channels = 3, out_channels = 64, norm_type = 'instance'))\n",
    "        self.dis  = init(Discriminator(in_channels = 3, out_channels = 64, norm_type = 'instance'))\n",
    "\n",
    "        self.d_params = self.dis.parameters()\n",
    "        self.g_params = self.gen.parameters()\n",
    "\n",
    "        self.example_input_array = torch.rand(1, 3, img_sz, img_sz, device = self.device)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def set_requires_grad(nets, requires_grad = False):\n",
    "\n",
    "        \"\"\"\n",
    "        Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
    "        Parameters:\n",
    "            nets (network list)   -- a list of networks\n",
    "            requires_grad (bool)  -- whether the networks require gradients or not\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(nets, list): nets = [nets]\n",
    "        for net in nets:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "    def forward(self, real_A):\n",
    "        \n",
    "        # this is different from the training step. You should treat this as the final inference code (final outputs that you are looking for!)\n",
    "        fake_B = self.gen(real_A)\n",
    "\n",
    "        return fake_B\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "\n",
    "        real_A, real_B = batch['A'], batch['B']\n",
    "        fake_B = self.gen(real_A)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # No need to calculate the gradients for Discriminators' parameters\n",
    "            self.set_requires_grad([self.dis], requires_grad = False)\n",
    "            dis_pred_fake_data = self.dis(torch.cat([real_A, fake_B], 0))\n",
    "            \n",
    "            # Gen loss\n",
    "            g_loss = self.loss.get_gen_loss(dis_pred_fake_data, real_B, fake_B)\n",
    "            self.log(\"g_train_loss\", g_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "            \n",
    "            return g_loss\n",
    "\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            self.set_requires_grad([self.dis], requires_grad = True)\n",
    "            dis_pred_real_data = self.dis(torch.cat([real_A, real_B], 0))\n",
    "            dis_pred_fake_data = self.dis(torch.cat([real_A, fake_B.detach()], 0))\n",
    "\n",
    "            # Dis loss\n",
    "            d_loss = self.loss.get_dis_loss(dis_pred_real_data, dis_pred_fake_data)\n",
    "            self.log(\"d_train_loss\", d_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "\n",
    "            return d_loss\n",
    "\n",
    "\n",
    "    def shared_step(self, batch, stage: str = 'val'):\n",
    "\n",
    "        grid = []\n",
    "        real_A, real_B = batch['A'], batch['B']\n",
    "\n",
    "        fake_B = self.gen(real_A)\n",
    "        dis_pred_fake_data = self.dis(torch.cat([real_A, fake_B], 0))\n",
    "        dis_pred_real_data = self.dis(torch.cat([real_A, real_B], 0))\n",
    "\n",
    "        # Gen loss, # Dis loss\n",
    "        g_loss = self.loss.get_gen_loss(dis_pred_fake_data, real_B, fake_B)\n",
    "        d_loss = self.loss.get_dis_loss(dis_pred_real_data, dis_pred_fake_data)\n",
    "\n",
    "        dict_ = {f'g_{stage}_loss': g_loss, f'd_{stage}_loss': d_loss}\n",
    "        self.log_dict(dict_, on_step = False, on_epoch = True, prog_bar = True, logger = True)\n",
    "\n",
    "        for i in range(12):\n",
    "            rand_int = np.random.randint(0, len(real_A))\n",
    "            tensor = torch.stack([real_A[rand_int], fake_B[rand_int], real_B[rand_int]])\n",
    "            grid.append((tensor + 1) / 2)\n",
    "            \n",
    "        # log the results on tensorboard\n",
    "        grid = torchvision.utils.make_grid(torch.cat(grid, 0), nrow = 6)\n",
    "        self.logger.experiment.add_image('Grid', grid, self.current_epoch, dataformats = \"CHW\")\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, 'val')\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, 'test')\n",
    "\n",
    "\n",
    "    def lr_lambda(self, epoch):\n",
    "\n",
    "        fraction = (epoch - self.epoch_decay) / self.epoch_decay\n",
    "        return 1 if epoch < self.epoch_decay else 1 - fraction\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # define the optimizers here\n",
    "        g_opt = torch.optim.Adam(self.g_params, lr = self.g_lr, betas = (self.beta_1, self.beta_2))\n",
    "        d_opt = torch.optim.Adam(self.d_params, lr = self.d_lr, betas = (self.beta_1, self.beta_2))\n",
    "        \n",
    "        # define the lr_schedulers here\n",
    "        g_sch = optim.lr_scheduler.LambdaLR(g_opt, lr_lambda = self.lr_lambda)\n",
    "        d_sch = optim.lr_scheduler.LambdaLR(d_opt, lr_lambda = self.lr_lambda)\n",
    "        \n",
    "        # first return value is a list of optimizers and second is a list of lr_schedulers (you can return empty list also)\n",
    "        return [g_opt, d_opt], [g_sch, d_sch]\n",
    "\n",
    "\n",
    "    \n",
    "###############################################################################################################################################\n",
    "\n",
    "\n",
    "TEST    = True\n",
    "TRAIN   = True\n",
    "RESTORE = False\n",
    "resume_from_checkpoint = None if TRAIN else \"path/to/checkpoints/\" # \"./logs/Pix2Pix/version_0/checkpoints/epoch=1.ckpt\"\n",
    "\n",
    "\n",
    "if TRAIN or RESTORE:\n",
    "    \n",
    "    epochs = 200\n",
    "    epoch_decay = epochs // 2\n",
    "    \n",
    "    model = Pix2Pix(epoch_decay = epoch_decay)\n",
    "    tb_logger = pl_loggers.TensorBoardLogger('logs/', name = \"Pix2Pix\", log_graph = True)\n",
    "    \n",
    "    lr_logger = LearningRateMonitor(logging_interval = 'epoch')\n",
    "    checkpoint_callback = ModelCheckpoint(monitor = \"g_val_loss\", save_top_k = 3, period = 2, save_last = True)\n",
    "    callbacks = [lr_logger, checkpoint_callback]\n",
    "    \n",
    "    # you can change the gpus argument to how many you have (I had only 1 :( )\n",
    "    # Setting deterministic flag to True for full reproducibility\n",
    "    trainer = pl.Trainer(accelerator = 'ddp', gpus = -1, max_epochs = epochs, progress_bar_refresh_rate = 20, precision = 16, \n",
    "                         callbacks = callbacks, num_sanity_val_steps = 1, logger = tb_logger, resume_from_checkpoint = \n",
    "                         resume_from_checkpoint, log_every_n_steps = 25, profiler = True, deterministic = True)\n",
    "    \n",
    "    trainer.fit(model, datamodule)\n",
    "    \n",
    "    \n",
    "if TEST:\n",
    "    \n",
    "    \"\"\"\n",
    "    This is one of the many ways to run inference, but I would recommend you to look into the docs for other \n",
    "    options as well, so that you can use one which suits you best.\n",
    "    \"\"\"\n",
    "    \n",
    "    trainer = pl.Trainer(gpus = -1, precision = 16, profiler = True)\n",
    "    # load the checkpoint that you want to load\n",
    "    checkpoint_path = \"path/to/checkpoints/\" # \"./logs/Pix2Pix/version_0/checkpoints/epoch=1.ckpt\"\n",
    "    \n",
    "    model = Pix2Pix.load_from_checkpoint(checkpoint_path = checkpoint_path)\n",
    "    model.freeze()\n",
    "    \n",
    "    # put the datamodule in test mode\n",
    "    datamodule.setup(\"test\")\n",
    "    test_data = datamodule.test_dataloader()\n",
    "\n",
    "    trainer.test(model, test_dataloaders = test_data)\n",
    "    # look tensorboard for the final results\n",
    "    # You can also run an inference on a single image using the forward function defined above!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
