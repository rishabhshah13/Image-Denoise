{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from CycleGANImports import *\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_image(image):\n",
        "    plt.imshow(np.transpose((image + 1) / 2, (1, 2, 0)))\n",
        "\n",
        "def get_random_sample(dataset):\n",
        "    return dataset[np.random.randint(0, len(dataset))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataModule(pl.LightningDataModule):\n",
        "\n",
        "    \"\"\"\n",
        "    Download the dataset using the below link; you just need to specify the url while creating an object of this class\n",
        "    https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\n",
        "    Authors don't follow a consistent format for all the datasets, so, it might not work for few\n",
        "\n",
        "    Implements the Lightining DataModule!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, url: str, root_dir: str = \"./Dataset/CycleGAN/\", img_sz: int = 256, trn_batch_sz: int = 4,\n",
        "                 tst_batch_sz: int = 64):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            url:          Download URL of the dataset\n",
        "            root_dir:     Root dir where dataset needs to be downloaded\n",
        "            img_sz:       Size of the Image\n",
        "            trn_batch_sz: Training Batch Size\n",
        "            tst_batch_sz: Test Batch Size\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.url = url\n",
        "        self.dataset = url.split(\"/\")[-1]\n",
        "\n",
        "        self.processed_dir  = root_dir + \"Processed/\"\n",
        "        self.compressed_dir = root_dir + \"Compressed/\"\n",
        "        os.makedirs(self.processed_dir , exist_ok = True)\n",
        "        os.makedirs(self.compressed_dir, exist_ok = True)\n",
        "\n",
        "        self.trn_batch_sz = trn_batch_sz\n",
        "        self.tst_batch_sz = tst_batch_sz\n",
        "\n",
        "        jitter_sz = int(img_sz * 1.120)\n",
        "        # self.tst_tfms = [Resize(img_sz), To_Tensor(), Normalize()]\n",
        "        # self.trn_tfms = [Resize(jitter_sz), RandomCrop(img_sz), Random_Flip(), To_Tensor(), Normalize()]\n",
        "\n",
        "\n",
        "    def prepare_data(self):\n",
        "\n",
        "        if self.dataset in os.listdir(self.compressed_dir):\n",
        "            print(f\"Dataset {self.dataset[:-4]} already exists!\")\n",
        "        else:\n",
        "            print(f\"Downloading dataset {self.dataset[:-4]}!!\")\n",
        "            wget.download(self.url, self.compressed_dir)\n",
        "            print(f\"\\nDataset {self.dataset[:-4]} downloaded. Extraction in progress!\")\n",
        "\n",
        "            with zipfile.ZipFile(self.compressed_dir + self.dataset, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.processed_dir)\n",
        "            print(f\"Extraction done!\")\n",
        "\n",
        "            # you might need to modify the below code; it's not generic, but works for most of the datasets listed in that url.\n",
        "            dwnld_dir = self.processed_dir + self.dataset[:-4] + \"/\"\n",
        "            for folder in [\"testA/\", \"testB/\", \"trainA/\", \"trainB/\"]:\n",
        "\n",
        "                dest_dir = dwnld_dir\n",
        "                src_dir  = dwnld_dir + folder\n",
        "\n",
        "                dest_dir = dest_dir + \"Train/\" if folder[:-2] != \"test\" else dest_dir + \"Test/\"\n",
        "                dest_dir = dest_dir + \"B/\"     if folder[-2]  != \"A\"    else dest_dir + \"A/\"\n",
        "                os.makedirs(dest_dir, exist_ok = True)\n",
        "\n",
        "                orig_files = [src_dir  + file for file in os.listdir(src_dir)]\n",
        "                modf_files = [dest_dir + \"{:06d}.jpg\".format(i) for i, file in enumerate(orig_files)]\n",
        "\n",
        "                for orig_file, modf_file in zip(orig_files, modf_files):\n",
        "                    shutil.move(orig_file, modf_file)\n",
        "                os.rmdir(src_dir)\n",
        "\n",
        "            print(f\"Files moved to appropiate folder!\")\n",
        "\n",
        "\n",
        "    def setup(self, stage: str = None):\n",
        "\n",
        "        \"\"\"\n",
        "        stage: fit/test\n",
        "        \"\"\"\n",
        "\n",
        "        dwnld_dir = self.processed_dir + self.dataset[:-4]\n",
        "        trn_dir = dwnld_dir + \"/Train/\"\n",
        "        tst_dir = dwnld_dir + \"/Test/\"\n",
        "\n",
        "        if stage == 'fit' or stage is None:\n",
        "\n",
        "            dataset = CustomDataset(path = trn_dir, transforms = self.trn_tfms)\n",
        "            train_sz = int(len(dataset) * 0.9)\n",
        "            valid_sz = len(dataset) - train_sz\n",
        "\n",
        "            self.train, self.valid = random_split(dataset, [train_sz, valid_sz])\n",
        "            print(f\"Size of the training dataset: {train_sz}, validation dataset: {valid_sz}\")\n",
        "\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.test = CustomDataset(path = tst_dir, transforms = self.tst_tfms)\n",
        "            print(f\"Size of the test dataset: {len(self.test)}\")\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train, batch_size = self.trn_batch_sz, shuffle = True , num_workers = 16, pin_memory = True)\n",
        "\n",
        "    def val_dataloader  (self):\n",
        "        return DataLoader(self.valid, batch_size = self.tst_batch_sz, shuffle = False, num_workers = 16, pin_memory = True)\n",
        "\n",
        "    def test_dataloader (self):\n",
        "        return DataLoader(self.test , batch_size = self.tst_batch_sz, shuffle = False, num_workers = 16, pin_memory = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# img_sz = 512\n",
        "# url = \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/cezanne2photo.zip\"\n",
        "\n",
        "# # You can decrease the num_workers argument in {train/val/test}_dataloader\n",
        "# datamodule = DataModule(url, trn_batch_sz = 1, tst_batch_sz = 64)\n",
        "# datamodule.prepare_data()\n",
        "# datamodule.setup(\"fit\")\n",
        "\n",
        "\n",
        "# print(f\"Few random samples from the Training dataset!\")\n",
        "\n",
        "# sample = get_random_sample(datamodule.train)\n",
        "# plt.subplot(1, 2, 1); show_image(sample['A'])\n",
        "# plt.subplot(1, 2, 2); show_image(sample['B'])\n",
        "# plt.show()\n",
        "\n",
        "# print(f\"Few random samples from the Validation dataset!\")\n",
        "\n",
        "# sample = get_random_sample(datamodule.valid)\n",
        "# plt.subplot(1, 2, 1); show_image(sample['A'])\n",
        "# plt.subplot(1, 2, 2); show_image(sample['B'])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels: int, apply_dp: bool = True):\n",
        "\n",
        "        \"\"\"\n",
        "                            Defines a ResBlock\n",
        "        X ------------------------identity------------------------\n",
        "        |-- Convolution -- Norm -- ReLU -- Convolution -- Norm --|\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            in_channels:  Number of input channels\n",
        "            apply_dp:     If apply_dp is set to True, then activations are 0'ed out with prob 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.Conv2d(in_channels = in_channels, out_channels = in_channels, kernel_size = 3, stride = 1)\n",
        "        layers =  [nn.ReflectionPad2d(1), conv, nn.InstanceNorm2d(in_channels), nn.ReLU(True)]\n",
        "\n",
        "        if apply_dp:\n",
        "            layers += [nn.Dropout(0.5)]\n",
        "\n",
        "        conv = nn.Conv2d(in_channels = in_channels, out_channels = in_channels, kernel_size = 3, stride = 1)\n",
        "        layers += [nn.ReflectionPad2d(1), conv, nn.InstanceNorm2d(in_channels)]\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x): return x + self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels: int = 3, out_channels: int = 64, apply_dp: bool = True):\n",
        "\n",
        "        \"\"\"\n",
        "                                Generator Architecture (Image Size: 256)\n",
        "        c7s1-64, d128, d256, R256, R256, R256, R256, R256, R256, R256, R256, R256, u128, u64, c7s1-3,\n",
        "\n",
        "        where c7s1-k denote a 7 × 7 Conv-InstanceNorm-ReLU layer with k filters and stride 1, dk denotes a 3 × 3\n",
        "        Conv-InstanceNorm-ReLU layer with k filters and stride 2, Rk denotes a residual block that contains two\n",
        "        3 × 3 Conv layers with the same number of filters on both layer. uk denotes a 3 × 3 DeConv-InstanceNorm-\n",
        "        ReLU layer with k filters and stride 1.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            in_channels:  Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            apply_dp:     If apply_dp is set to True, then activations are 0'ed out with prob 0.5\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        f = 1\n",
        "        nb_downsampling = 2\n",
        "        nb_resblks = 6 if img_sz == 128 else 9\n",
        "\n",
        "        conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 7, stride = 1)\n",
        "        self.layers = [nn.ReflectionPad2d(3), conv, nn.InstanceNorm2d(out_channels), nn.ReLU(True)]\n",
        "\n",
        "        for i in range(nb_downsampling):\n",
        "            conv = nn.Conv2d(out_channels * f, out_channels * 2 * f, kernel_size = 3, stride = 2, padding = 1)\n",
        "            self.layers += [conv, nn.InstanceNorm2d(out_channels * 2 * f), nn.ReLU(True)]\n",
        "            f *= 2\n",
        "\n",
        "        for i in range(nb_resblks):\n",
        "            res_blk = ResBlock(in_channels = out_channels * f, apply_dp = apply_dp)\n",
        "            self.layers += [res_blk]\n",
        "\n",
        "        for i in range(nb_downsampling):\n",
        "            conv = nn.ConvTranspose2d(out_channels * f, out_channels * (f//2), 3, 2, padding = 1, output_padding = 1)\n",
        "            self.layers += [conv, nn.InstanceNorm2d(out_channels * (f//2)), nn.ReLU(True)]\n",
        "            f = f // 2\n",
        "\n",
        "        conv = nn.Conv2d(in_channels = out_channels, out_channels = in_channels, kernel_size = 7, stride = 1)\n",
        "        self.layers += [nn.ReflectionPad2d(3), conv, nn.Tanh()]\n",
        "\n",
        "        self.net = nn.Sequential(*self.layers)\n",
        "\n",
        "\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels: int = 3, out_channels: int = 64, nb_layers: int = 3):\n",
        "\n",
        "        \"\"\"\n",
        "                                    Discriminator Architecture!\n",
        "        C64 - C128 - C256 - C512, where Ck denote a Convolution-InstanceNorm-LeakyReLU layer with k filters\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            in_channels:    Number of input channels\n",
        "            out_channels:   Number of output channels\n",
        "            nb_layers:      Number of layers in the 70*70 Patch Discriminator\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        in_f  = 1\n",
        "        out_f = 2\n",
        "\n",
        "        conv = nn.Conv2d(in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1)\n",
        "        self.layers = [conv, nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        for idx in range(1, nb_layers):\n",
        "            conv = nn.Conv2d(out_channels * in_f, out_channels * out_f, kernel_size = 4, stride = 2, padding = 1)\n",
        "            self.layers += [conv, nn.InstanceNorm2d(out_channels * out_f), nn.LeakyReLU(0.2, True)]\n",
        "            in_f   = out_f\n",
        "            out_f *= 2\n",
        "\n",
        "        out_f = min(2 ** nb_layers, 8)\n",
        "        conv = nn.Conv2d(out_channels * in_f,  out_channels * out_f, kernel_size = 4, stride = 1, padding = 1)\n",
        "        self.layers += [conv, nn.InstanceNorm2d(out_channels * out_f), nn.LeakyReLU(0.2, True)]\n",
        "\n",
        "        conv = nn.Conv2d(out_channels * out_f, out_channels = 1, kernel_size = 4, stride = 1, padding = 1)\n",
        "        self.layers += [conv]\n",
        "\n",
        "        self.net = nn.Sequential(*self.layers)\n",
        "\n",
        "\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class Initializer:\n",
        "\n",
        "    def __init__(self, init_type: str = 'normal', init_gain: float = 0.02):\n",
        "\n",
        "        \"\"\"\n",
        "        Initializes the weight of the network!\n",
        "\n",
        "        Parameters:\n",
        "            init_type: Initializer type - 'kaiming' or 'xavier' or 'normal'\n",
        "            init_gain: Standard deviation of the normal distribution\n",
        "        \"\"\"\n",
        "\n",
        "        self.init_type = init_type\n",
        "        self.init_gain = init_gain\n",
        "\n",
        "\n",
        "    def init_module(self, m):\n",
        "\n",
        "        cls_name = m.__class__.__name__;\n",
        "        if hasattr(m, 'weight') and (cls_name.find('Conv') != -1 or cls_name.find('Linear') != -1):\n",
        "\n",
        "            if   self.init_type == 'kaiming': nn.init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif self.init_type == 'xavier' : nn.init.xavier_normal_ (m.weight.data,  gain = self.init_gain)\n",
        "            elif self.init_type == 'normal' : nn.init.normal_(m.weight.data, mean = 0, std = self.init_gain)\n",
        "            else: raise ValueError('Initialization not found!!')\n",
        "\n",
        "            if m.bias is not None: nn.init.constant_(m.bias.data, val = 0);\n",
        "\n",
        "        if hasattr(m, 'weight') and cls_name.find('BatchNorm2d') != -1:\n",
        "            nn.init.normal_(m.weight.data, mean = 1.0, std = self.init_gain)\n",
        "            nn.init.constant_(m.bias.data, val = 0)\n",
        "\n",
        "\n",
        "    def __call__(self, net):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            net: Network\n",
        "        \"\"\"\n",
        "\n",
        "        net.apply(self.init_module)\n",
        "\n",
        "        return net\n",
        "\n",
        "\n",
        "\n",
        "class ImagePool:\n",
        "\n",
        "    \"\"\"\n",
        "    This class implements an image buffer that stores previously generated images! This buffer enables to update\n",
        "    discriminators using a history of generated image rather than the latest ones produced by generator.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pool_sz: int = 50):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            pool_sz: Size of the image buffer\n",
        "        \"\"\"\n",
        "\n",
        "        self.nb_images = 0\n",
        "        self.image_pool = []\n",
        "        self.pool_sz = pool_sz\n",
        "\n",
        "\n",
        "    def push_and_pop(self, images):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            images: latest images generated by the generator\n",
        "\n",
        "        Returns a batch of images from pool!\n",
        "        \"\"\"\n",
        "\n",
        "        images_to_return = []\n",
        "        for image in images:\n",
        "            image = torch.unsqueeze(image, 0)\n",
        "\n",
        "            if  self.nb_images < self.pool_sz:\n",
        "                self.image_pool.append (image)\n",
        "                images_to_return.append(image)\n",
        "                self.nb_images += 1\n",
        "            else:\n",
        "                if np.random.uniform(0, 1) > 0.5:\n",
        "\n",
        "                    rand_int = np.random.randint(0, self.pool_sz)\n",
        "                    temp_img = self.image_pool[rand_int].clone()\n",
        "                    self.image_pool[rand_int] = image\n",
        "                    images_to_return.append(temp_img)\n",
        "                else:\n",
        "                    images_to_return.append(image)\n",
        "\n",
        "        return torch.cat(images_to_return, 0)\n",
        "\n",
        "\n",
        "\n",
        "class Loss:\n",
        "\n",
        "    \"\"\"\n",
        "    This class implements different losses required to train the generators and discriminators of CycleGAN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, loss_type: str = 'MSE', lambda_: int = 10):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            loss_type: Loss Function to train CycleGAN\n",
        "            lambda_:   Weightage of Cycle-consistency loss\n",
        "        \"\"\"\n",
        "\n",
        "        self.loss = nn.MSELoss() if loss_type == 'MSE' else nn.BCEWithLogitsLoss()\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "\n",
        "    def get_dis_loss(self, dis_pred_real_data, dis_pred_fake_data):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            dis_pred_real_data: Discriminator's prediction on real data\n",
        "            dis_pred_fake_data: Discriminator's prediction on fake data\n",
        "        \"\"\"\n",
        "\n",
        "        dis_tar_real_data = torch.ones_like (dis_pred_real_data, requires_grad = False)\n",
        "        dis_tar_fake_data = torch.zeros_like(dis_pred_fake_data, requires_grad = False)\n",
        "\n",
        "        loss_real_data = self.loss(dis_pred_real_data, dis_tar_real_data)\n",
        "        loss_fake_data = self.loss(dis_pred_fake_data, dis_tar_fake_data)\n",
        "\n",
        "        dis_tot_loss = (loss_real_data + loss_fake_data) * 0.5\n",
        "\n",
        "        return dis_tot_loss\n",
        "\n",
        "\n",
        "    def get_gen_gan_loss(self, dis_pred_fake_data):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            dis_pred_fake_data: Discriminator's prediction on fake data\n",
        "        \"\"\"\n",
        "\n",
        "        gen_tar_fake_data = torch.ones_like(dis_pred_fake_data, requires_grad = False)\n",
        "        gen_tot_loss = self.loss(dis_pred_fake_data, gen_tar_fake_data)\n",
        "\n",
        "        return gen_tot_loss\n",
        "\n",
        "\n",
        "    def get_gen_cyc_loss(self, real_data, cyc_data):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            real_data: Real images sampled from the dataloaders\n",
        "            cyc_data:  Image reconstructed after passing the real image through both the generators\n",
        "                       X_recons = F * G (X_real), where F and G are the two generators\n",
        "        \"\"\"\n",
        "\n",
        "        gen_cyc_loss = torch.nn.L1Loss()(real_data, cyc_data)\n",
        "        gen_tot_loss = gen_cyc_loss * self.lambda_\n",
        "\n",
        "        return gen_tot_loss\n",
        "\n",
        "\n",
        "    def get_gen_idt_loss(self, real_data, idt_data):\n",
        "\n",
        "        \"\"\"\n",
        "        Implements the identity loss:\n",
        "            nn.L1Loss(LG_B2A(real_A), real_A)\n",
        "            nn.L1Loss(LG_A2B(real_B), real_B)\n",
        "        \"\"\"\n",
        "\n",
        "        gen_idt_loss = torch.nn.L1Loss()(real_data, idt_data)\n",
        "        gen_tot_loss = gen_idt_loss * self.lambda_ * 0.5\n",
        "\n",
        "        return gen_tot_loss\n",
        "\n",
        "\n",
        "    def get_gen_loss(self, real_A, real_B, cyc_A, cyc_B, idt_A, idt_B, d_A_pred_fake_data,\n",
        "                     d_B_pred_fake_data):\n",
        "\n",
        "        \"\"\"\n",
        "        Implements the total Generator loss\n",
        "        Sum of Cycle loss, Identity loss, and GAN loss\n",
        "        \"\"\"\n",
        "\n",
        "        #Cycle loss\n",
        "        cyc_loss_A = self.get_gen_cyc_loss(real_A, cyc_A)\n",
        "        cyc_loss_B = self.get_gen_cyc_loss(real_B, cyc_B)\n",
        "        tot_cyc_loss = cyc_loss_A + cyc_loss_B\n",
        "\n",
        "        # GAN loss\n",
        "        g_A2B_gan_loss = self.get_gen_gan_loss(d_B_pred_fake_data)\n",
        "        g_B2A_gan_loss = self.get_gen_gan_loss(d_A_pred_fake_data)\n",
        "\n",
        "        # Identity loss\n",
        "        g_B2A_idt_loss = self.get_gen_idt_loss(real_A, idt_A)\n",
        "        g_A2B_idt_loss = self.get_gen_idt_loss(real_B, idt_B)\n",
        "\n",
        "        # Total individual losses\n",
        "        g_A2B_loss = g_A2B_gan_loss + g_A2B_idt_loss + tot_cyc_loss\n",
        "        g_B2A_loss = g_B2A_gan_loss + g_B2A_idt_loss + tot_cyc_loss\n",
        "        g_tot_loss = g_A2B_loss + g_B2A_loss - tot_cyc_loss\n",
        "\n",
        "        return g_A2B_loss, g_B2A_loss, g_tot_loss\n",
        "\n",
        "\n",
        "\n",
        "def display_progress(cond, real, fake, figsize=(10,5)):\n",
        "    cond = cond.detach().cpu().permute(1, 2, 0)\n",
        "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
        "    real = real.detach().cpu().permute(1, 2, 0)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
        "    ax[0].imshow(cond)\n",
        "    ax[1].imshow(real)\n",
        "    ax[2].imshow(fake)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class CycleGAN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, d_lr: float = 2e-4, g_lr: float = 2e-4, beta_1: float = 0.5, beta_2: float = 0.999, \n",
        "                 epoch_decay: int = 200 ,img_sz: int = 256):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "        self.d_lr = d_lr\n",
        "        self.g_lr = g_lr\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "        self.epoch_decay = epoch_decay\n",
        "        self.img_sz = img_sz\n",
        "\n",
        "        self.fake_pool_A = ImagePool(pool_sz = 50)\n",
        "        self.fake_pool_B = ImagePool(pool_sz = 50)\n",
        "\n",
        "        self.loss = Loss(loss_type = 'MSE', lambda_ = 10)\n",
        "        init = Initializer(init_type = 'normal', init_gain = 0.02)\n",
        "\n",
        "        self.d_A = init(Discriminator(in_channels = 3, out_channels = 64, nb_layers = 3))\n",
        "        self.d_B = init(Discriminator(in_channels = 3, out_channels = 64, nb_layers = 3))\n",
        "        self.g_A2B = init(Generator(in_channels = 3, out_channels = 64, apply_dp = False))\n",
        "        self.g_B2A = init(Generator(in_channels = 3, out_channels = 64, apply_dp = False))\n",
        "\n",
        "        self.d_A_params = self.d_A.parameters()\n",
        "        self.d_B_params = self.d_B.parameters()\n",
        "        self.g_params   = itertools.chain([*self.g_A2B.parameters(), *self.g_B2A.parameters()])\n",
        "\n",
        "        self.example_input_array = [torch.rand(1, 3, self.img_sz, self.img_sz, device = self.device),\n",
        "                                    torch.rand(1, 3, self.img_sz, self.img_sz, device = self.device)]\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def set_requires_grad(nets, requires_grad = False):\n",
        "\n",
        "        \"\"\"\n",
        "        Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
        "        Parameters:\n",
        "            nets (network list)   -- a list of networks\n",
        "            requires_grad (bool)  -- whether the networks require gradients or not\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(nets, list): nets = [nets]\n",
        "        for net in nets:\n",
        "            for param in net.parameters():\n",
        "                param.requires_grad = requires_grad\n",
        "\n",
        "\n",
        "    def forward(self, real_A, real_B):\n",
        "        \n",
        "        \"\"\"\n",
        "        This is different from the training step. You should treat this as the final inference code \n",
        "        (final outputs that you are looking for!), but you can definitely use it in the training_step \n",
        "        to make some code reusable.\n",
        "        Parameters:\n",
        "            real_A -- real image of A\n",
        "            real_B -- real image of B\n",
        "        \"\"\"\n",
        "        \n",
        "        fake_B = self.g_A2B(real_A)\n",
        "        fake_A = self.g_B2A(real_B)\n",
        "\n",
        "        return fake_B, fake_A\n",
        "    \n",
        "    \n",
        "    def forward_gen(self, real_A, real_B, fake_A, fake_B):\n",
        "        \n",
        "        \"\"\"\n",
        "        Gets the remaining output of both the generators for the training/validation step\n",
        "        Parameters:\n",
        "            real_A -- real image of A\n",
        "            real_B -- real image of B\n",
        "            fake_A -- fake image of A\n",
        "            fake_B -- fake image of B\n",
        "        \"\"\"\n",
        "        \n",
        "        cyc_A = self.g_B2A(fake_B)\n",
        "        idt_A = self.g_B2A(real_A)\n",
        "        \n",
        "        cyc_B = self.g_A2B(fake_A)\n",
        "        idt_B = self.g_A2B(real_B)\n",
        "        \n",
        "        return cyc_A, idt_A, cyc_B, idt_B\n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    def forward_dis(dis, real_data, fake_data):\n",
        "        \n",
        "        \"\"\"\n",
        "        Gets the Discriminator output\n",
        "        Parameters:\n",
        "            dis       -- Discriminator\n",
        "            real_data -- real image\n",
        "            fake_data -- fake image\n",
        "        \"\"\"\n",
        "        \n",
        "        pred_real_data = dis(real_data)\n",
        "        pred_fake_data = dis(fake_data)\n",
        "        \n",
        "        return pred_real_data, pred_fake_data\n",
        "\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        # real_A, real_B = batch['A'], batch['B']\n",
        "        real_A, real_B = batch\n",
        "        fake_B, fake_A = self(real_A, real_B)\n",
        "        \n",
        "\n",
        "        g_opt, d_A_opt, d_B_opt = self.optimizers()\n",
        "        \n",
        "        # if optimizer_idx == 0:\n",
        "            \n",
        "        cyc_A, idt_A, cyc_B, idt_B = self.forward_gen(real_A, real_B, fake_A, fake_B)\n",
        "        \n",
        "        # No need to calculate the gradients for Discriminators' parameters\n",
        "        self.set_requires_grad([self.d_A, self.d_B], requires_grad = False)\n",
        "        d_A_pred_fake_data = self.d_A(fake_A)\n",
        "        d_B_pred_fake_data = self.d_B(fake_B)\n",
        "\n",
        "        g_A2B_loss, g_B2A_loss, g_tot_loss = self.loss.get_gen_loss(real_A, real_B, cyc_A, cyc_B, idt_A, idt_B, \n",
        "                                                                    d_A_pred_fake_data, d_B_pred_fake_data)\n",
        "\n",
        "        dict_ = {'g_tot_train_loss': g_tot_loss, 'g_A2B_train_loss': g_A2B_loss, 'g_B2A_train_loss': g_B2A_loss}\n",
        "        self.log_dict(dict_, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "        self.manual_backward(g_tot_loss)\n",
        "        g_opt.step() \n",
        "\n",
        "            # return g_tot_loss\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        # if optimizer_idx == 1:\n",
        "            \n",
        "        self.set_requires_grad([self.d_A], requires_grad = True)\n",
        "        fake_A = self.fake_pool_A.push_and_pop(fake_A)\n",
        "        d_A_pred_real_data, d_A_pred_fake_data = self.forward_dis(self.d_A, real_A, fake_A.detach())\n",
        "\n",
        "        # GAN loss\n",
        "        d_A_loss = self.loss.get_dis_loss(d_A_pred_real_data, d_A_pred_fake_data)\n",
        "        self.log(\"d_A_train_loss\", d_A_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "        self.manual_backward(d_A_loss)\n",
        "        d_A_opt.step() \n",
        "\n",
        "            # return d_A_loss\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        # if optimizer_idx == 2:\n",
        "        \n",
        "        self.set_requires_grad([self.d_B], requires_grad = True)\n",
        "        fake_B = self.fake_pool_B.push_and_pop(fake_B)\n",
        "        d_B_pred_real_data, d_B_pred_fake_data = self.forward_dis(self.d_B, real_B, fake_B.detach())\n",
        "\n",
        "        # GAN loss\n",
        "        d_B_loss = self.loss.get_dis_loss(d_B_pred_real_data, d_B_pred_fake_data)\n",
        "        self.log(\"d_B_train_loss\", d_B_loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "\n",
        "        self.manual_backward(d_B_loss)\n",
        "        d_B_opt.step() \n",
        "\n",
        "            # return d_B_loss\n",
        "\n",
        "\n",
        "\n",
        "        display_step=25\n",
        "        if self.current_epoch%display_step==0 and batch_idx==0:\n",
        "            fake_A, fake_B = self(real_A, real_B)\n",
        "            # display_progress(condition[0], fake[0], real[0])\n",
        "            display_progress(real_A[0], real_A[0], fake_A[0])\n",
        "            display_progress(real_B[0], real_B[0], fake_B[0])\n",
        "\n",
        "\n",
        "\n",
        "    def shared_step(self, batch, stage: str = 'val'):\n",
        "\n",
        "        grid_A = []\n",
        "        grid_B = []\n",
        "        \n",
        "        # real_A, real_B = batch['A'], batch['B']\n",
        "        real_A, real_B = batch\n",
        "        \n",
        "        fake_B, fake_A = self(real_A, real_B)\n",
        "        cyc_A , idt_A , cyc_B, idt_B = self.forward_gen(real_A, real_B, fake_A, fake_B)\n",
        "        \n",
        "        d_A_pred_real_data, d_A_pred_fake_data = self.forward_dis(self.d_A, real_A, fake_A)\n",
        "        d_B_pred_real_data, d_B_pred_fake_data = self.forward_dis(self.d_B, real_B, fake_B)\n",
        "        \n",
        "        # G_A2B loss, G_B2A loss, G loss\n",
        "        g_A2B_loss, g_B2A_loss, g_tot_loss = self.loss.get_gen_loss(real_A, real_B, cyc_A, cyc_B, idt_A, idt_B, \n",
        "                                                                    d_A_pred_fake_data, d_B_pred_fake_data)\n",
        "\n",
        "        # D_A loss, D_B loss\n",
        "        d_A_loss = self.loss.get_dis_loss(d_A_pred_real_data, d_A_pred_fake_data)\n",
        "        d_B_loss = self.loss.get_dis_loss(d_B_pred_real_data, d_B_pred_fake_data)\n",
        "\n",
        "        dict_ = {f'g_tot_{stage}_loss': g_tot_loss, f'g_A2B_{stage}_loss': g_A2B_loss, f'g_B2A_{stage}_loss': g_B2A_loss, \n",
        "                 f'd_A_{stage}_loss'  : d_A_loss  , f'd_B_{stage}_loss'  : d_B_loss}\n",
        "        self.log_dict(dict_, on_step = False, on_epoch = True, prog_bar = True, logger = True)\n",
        "\n",
        "        for i in range(12):\n",
        "            rand_int = np.random.randint(0, len(real_A))\n",
        "            tensor = torch.stack([real_A[rand_int], fake_B[rand_int], cyc_A[rand_int],\n",
        "                                  real_B[rand_int], fake_A[rand_int], cyc_B[rand_int]])\n",
        "            tensor = (tensor + 1) / 2\n",
        "            grid_A.append(tensor[:3])\n",
        "            grid_B.append(tensor[3:])\n",
        "        \n",
        "        # log the results on tensorboard\n",
        "        grid_A = torchvision.utils.make_grid(torch.cat(grid_A, 0), nrow = 6)\n",
        "        grid_B = torchvision.utils.make_grid(torch.cat(grid_B, 0), nrow = 6)\n",
        "        self.logger.experiment.add_image('Grid_A', grid_A, self.current_epoch, dataformats = \"CHW\")\n",
        "        self.logger.experiment.add_image('Grid_B', grid_B, self.current_epoch, dataformats = \"CHW\")\n",
        "\n",
        "\n",
        "        # display_progress(condition[0], fake[0], real[0])\n",
        "        # for \n",
        "        # display_progress(real_A[0], real_A[0], fake_A[0])\n",
        "        # display_progress(real_B[0], real_B[0], fake_B[0])\n",
        "        for i in range(real_A.shape[0]):\n",
        "            display_progress(real_A[i], real_A[i], fake_A[i])\n",
        "            display_progress(real_B[i], real_B[i], fake_B[i])\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.shared_step(batch, 'val')\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        a,b = batch\n",
        "        # print(\"TESTING\"*100)\n",
        "        return self.shared_step(batch, 'test')\n",
        "\n",
        "\n",
        "    def lr_lambda(self, epoch):\n",
        "\n",
        "        fraction = (epoch - self.epoch_decay) / self.epoch_decay\n",
        "        return 1 if epoch < self.epoch_decay else 1 - fraction\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \n",
        "        # define the optimizers here\n",
        "        g_opt   = torch.optim.Adam(self.g_params  , lr = self.g_lr, betas = (self.beta_1, self.beta_2))\n",
        "        d_A_opt = torch.optim.Adam(self.d_A_params, lr = self.d_lr, betas = (self.beta_1, self.beta_2))\n",
        "        d_B_opt = torch.optim.Adam(self.d_B_params, lr = self.d_lr, betas = (self.beta_1, self.beta_2))\n",
        "        \n",
        "        # define the lr_schedulers here\n",
        "        g_sch   = optim.lr_scheduler.LambdaLR(g_opt  , lr_lambda = self.lr_lambda)\n",
        "        d_A_sch = optim.lr_scheduler.LambdaLR(d_A_opt, lr_lambda = self.lr_lambda)\n",
        "        d_B_sch = optim.lr_scheduler.LambdaLR(d_B_opt, lr_lambda = self.lr_lambda)\n",
        "        \n",
        "        # first return value is a list of optimizers and second is a list of lr_schedulers \n",
        "        # (you can return empty list also)\n",
        "        return [g_opt, d_A_opt, d_B_opt], [g_sch, d_A_sch, d_B_sch]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cl =CycleGAN()\n",
        "\n",
        "# for batch in dataloader:\n",
        "#     real_A, real_B = batch\n",
        "#     fake_B, fake_A = cl(real_A, real_B)\n",
        "#     break\n",
        "\n",
        "# for i in range(real_A.shape[0]):\n",
        "#     display_progress(real_A[i], real_A[i], fake_A[i])\n",
        "#     display_progress(real_B[i], real_B[i], fake_B[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "# Example usage:\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize the image\n",
        "    transforms.ToTensor(),          # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "root_dir='SIDD_Small_sRGB/SIDD_Small_sRGB/Data/'\n",
        "base_filename = 'SIDD_Small_sRGB'\n",
        "\n",
        "data_loader_manager = DataLoaderManager(root_dir=root_dir, train_file=base_filename,test_file=base_filename,make_held_out_set=False,transform=transform)\n",
        "dataloader, val_dataloader = data_loader_manager.process_dataloaders(batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "TEST    = True\n",
        "TRAIN   = True\n",
        "RESTORE = False\n",
        "resume_from_checkpoint = None if TRAIN else \"lightning_logs/CycleGAN/\" # \"./logs/CycleGAN/version_0/checkpoints/epoch=1.ckpt\"\n",
        "\n",
        "img_sz = 256\n",
        "\n",
        "if TRAIN or RESTORE:\n",
        "    \n",
        "    epochs = 500\n",
        "    epoch_decay = epochs // 2\n",
        "    \n",
        "    model = CycleGAN(epoch_decay = epoch_decay)\n",
        "    tb_logger = pl_loggers.TensorBoardLogger('lightning_logs/CycleGAN/logs/', name = \"CycleGAN\", log_graph = True)\n",
        "    \n",
        "    lr_logger = LearningRateMonitor(logging_interval = 'epoch')\n",
        "    # checkpoint_callback = ModelCheckpoint(monitor = \"g_tot_val_loss\", save_top_k = 3, period = 2, save_last = True)\n",
        "    # callbacks = [lr_logger, checkpoint_callback]\n",
        "    \n",
        "    # you can change the gpus argument to how many you have (I had only 1 :( )\n",
        "    # Set the deterministic flag to True for full reproducibility\n",
        "    trainer = pl.Trainer(max_epochs = epochs)\n",
        "    \n",
        "    trainer.fit(model, dataloader)\n",
        "\n",
        "if TEST:\n",
        "    \n",
        "    \"\"\"\n",
        "    This is one of the many ways to run inference, but I would recommend you to look into the docs for other \n",
        "    options as well, so that you can use one which suits you best.\n",
        "    \"\"\"\n",
        "    \n",
        "    trainer = pl.Trainer()\n",
        "    # load the checkpoint that you want to load\n",
        "    # checkpoint_path = \"/lightning_logs/version_0/checkpoints/epoch=49-step=1950.ckpt\" # \"./logs/CycleGAN/version_0/checkpoints/epoch=1.ckpt\"\n",
        "    checkpoint_path = \"lightning_logs/version_0/checkpoints/epoch=49-step=1950.ckpt\" # \"./logs/CycleGAN/version_0/checkpoints/epoch=1.ckpt\"\n",
        "    \n",
        "    \n",
        "    model = CycleGAN.load_from_checkpoint(checkpoint_path = checkpoint_path)\n",
        "    model.freeze()\n",
        "    \n",
        "    # put the datamodule in test mode\n",
        "    # datamodule.setup(\"test\")\n",
        "    # test_data = datamodule.test_dataloader()\n",
        "\n",
        "    trainer.test(model, dataloaders = val_dataloader)\n",
        "    # look tensorboard for the final results\n",
        "    # You can also run an inference on a single image using the forward function defined above!!\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PbGW4pRoYqRy"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1cadc9681bb64b94a360413affc54fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "201abdb5733c4fbc96495c97bd622726": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31e499dee226485f89d9c9b073f012ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59548eaae4dc4f4f96f25259b40deae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e499dee226485f89d9c9b073f012ae",
            "placeholder": "​",
            "style": "IPY_MODEL_65f26eb6de1a42be864ba4668dd0286b",
            "value": "Epoch 92:   0%"
          }
        },
        "5cc30ceb83fa48a1b1e306de335cc0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65f26eb6de1a42be864ba4668dd0286b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "709a32ea4a2841019d2b626102414b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cadc9681bb64b94a360413affc54fd3",
            "placeholder": "​",
            "style": "IPY_MODEL_5cc30ceb83fa48a1b1e306de335cc0a5",
            "value": " 0/4 [00:00&lt;?, ?it/s, v_num=5]"
          }
        },
        "84b397a5070142aeb7889e8bc6c37d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f777fdcd7fa340c7bb7bb1020f6cf6cc",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_201abdb5733c4fbc96495c97bd622726",
            "value": 0
          }
        },
        "b1056d32ba424177b559291396289a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59548eaae4dc4f4f96f25259b40deae3",
              "IPY_MODEL_84b397a5070142aeb7889e8bc6c37d02",
              "IPY_MODEL_709a32ea4a2841019d2b626102414b7f"
            ],
            "layout": "IPY_MODEL_de5fd2115852487d80d967f1f69e6b9c"
          }
        },
        "de5fd2115852487d80d967f1f69e6b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "f777fdcd7fa340c7bb7bb1020f6cf6cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
